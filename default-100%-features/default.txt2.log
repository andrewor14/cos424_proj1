preprocessSentences.py:68: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal
  return [clean_word(w) for w in tokens if w.lower() not in stop_words and w.isalpha()]
Path: .
Training data: ./default.txt2.train
Done building things up man. Num documents in train set: 2400.
Number of features before any feature selection: 10596
Number of features after filtering out words by count threshold: 514
Done doing the feature selection thing man. Num vocabs: 514.
Output files: ./out*
Runtime: 3.86357498169
Running model 'mybnb' on train data 'default.txt2.train' and test data 'default.txt2.test'
Num train examples = 2400, positive prior = 0.541666666667, negative prior = 0.458333333333
Using 514 features: second, unit, music, hold, first tim, want, hot, wrong, beauti, fit...
493 features are used in positive reviews: forget, lack, month, abil, go, tv, friendli, send, certainli, worth...
476 features are used in negative reviews: forget, lack, month, abil, go, hate, tv, friendli, send, certainli...
------------------------------------------------------------------------
Classifying line:
  601	Magical Help.	1
  words = [magic, help, magic help]
  positive probability = 1.71789730317e-09, negative probability = 1.71240569786e-09
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  602	The best phone in market :).	1
  words = [best, phone, market, best phone, phone market]
  positive probability = 2.31631261326e-13, negative probability = 5.33009899333e-14
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  603	It worked very well.	1
  words = [work, well, work well]
  positive probability = 3.36707871422e-07, negative probability = 5.75368314479e-08
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  604	The company shipped my product very promptly and it works well.	1
  words = [compani, ship, product, promptli, work, well, compani ship, ship product, product promptli, promptli work, work well]
  positive probability = 9.78535615368e-31, negative probability = 2.3808700668e-30
  predicted label = 0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  605	Exactly what I wanted.	1
  words = [exactli, want, exactli want]
  positive probability = 2.35597344435e-08, negative probability = 5.82217937271e-09
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  606	This is a great deal.	1
  words = [great, deal, great deal]
  positive probability = 1.20743639023e-07, negative probability = 4.10977367485e-09
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  607	Excellent product, I am very satisfied with the purchase.	1
  words = [excel, product, satisfi, purchas, excel product, product satisfi, satisfi purchas]
  positive probability = 3.68925749102e-20, negative probability = 6.96677101902e-21
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  608	I highly recommend these and encourage people to give them a try.	1
  words = [highli, recommend, encourag, peopl, give, tri, highli recommend, recommend encourag, encourag peopl, peopl give, give tri]
  positive probability = 4.7010897899e-28, negative probability = 1.79585627896e-29
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  609	Better than you'd expect.	1
  words = [better, expect, better expect]
  positive probability = 1.96331120363e-08, negative probability = 8.01405866596e-08
  predicted label = 0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  610	Great product and price.	1
  words = [great, product, price, great product, product price]
  positive probability = 3.84623709432e-12, negative probability = 5.92233221481e-14
  predicted label = 1, expected label = 1

================================
You guessed 396/600 = 66.0% correct.
  - False positive rate: [0.0, 0.29, 1.0]
  - True positive rate: [0.0, 0.56, 1.0]
  - Thresholds: [2, 1, 0]
  - AUC: 0.635
  - F1: 0.523364485981
================================

Running model 'mymnb' on train data 'default.txt2.train' and test data 'default.txt2.test'
Num train examples = 2400, positive prior = 0.541666666667, negative prior = 0.458333333333
Using 514 features: second, unit, music, hold, first tim, want, hot, wrong, beauti, fit...
493 features are used in positive reviews: forget, lack, month, abil, go, tv, friendli, send, certainli, worth...
476 features are used in negative reviews: forget, lack, month, abil, go, hate, tv, friendli, send, certainli...
------------------------------------------------------------------------
Classifying line:
  601	Magical Help.	1
  words = [magic, help, magic help]
  positive probability = 6.35211427439e-10, negative probability = 5.45055663507e-10
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  602	The best phone in market :).	1
  words = [best, phone, market, best phone, phone market]
  positive probability = 5.16239868386e-14, negative probability = 8.16015490111e-15
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  603	It worked very well.	1
  words = [work, well, work well]
  positive probability = 1.36842690368e-07, negative probability = 1.83138702938e-08
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  604	The company shipped my product very promptly and it works well.	1
  words = [compani, ship, product, promptli, work, well, compani ship, ship product, product promptli, promptli work, work well]
  positive probability = 2.80113850765e-32, negative probability = 3.57926950556e-32
  predicted label = 0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  605	Exactly what I wanted.	1
  words = [exactli, want, exactli want]
  positive probability = 8.71147100488e-09, negative probability = 1.96220038863e-09
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  606	This is a great deal.	1
  words = [great, deal, great deal]
  positive probability = 4.68241566512e-08, negative probability = 1.30813359242e-09
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  607	Excellent product, I am very satisfied with the purchase.	1
  words = [excel, product, satisfi, purchas, excel product, product satisfi, satisfi purchas]
  positive probability = 3.82151856901e-21, negative probability = 4.81923368145e-22
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  608	I highly recommend these and encourage people to give them a try.	1
  words = [highli, recommend, encourag, peopl, give, tri, highli recommend, recommend encourag, encourag peopl, peopl give, give tri]
  positive probability = 1.30598439122e-29, negative probability = 2.69979185562e-31
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  609	Better than you'd expect.	1
  words = [better, expect, better expect]
  positive probability = 8.16700406708e-09, negative probability = 2.55086050521e-08
  predicted label = 0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  610	Great product and price.	1
  words = [great, product, price, great product, product price]
  positive probability = 7.68403188713e-13, negative probability = 8.78785912428e-15
  predicted label = 1, expected label = 1

================================
You guessed 384/600 = 64.0% correct.
  - False positive rate: [0.0, 0.365, 1.0]
  - True positive rate: [0.0, 0.65, 1.0]
  - Thresholds: [2, 1, 0]
  - AUC: 0.6425
  - F1: 0.546218487395
================================

Running model 'bnb' on train data 'default.txt2.train' and test data 'default.txt2.test'
------------------------------------------------------------------------
Classifying line:
  601	Magical Help.	1
  words = [magic, help, magic help]
  predicted label = 0.55099664953, expected label = 1
------------------------------------------------------------------------
Classifying line:
  602	The best phone in market :).	1
  words = [best, phone, market, best phone, phone market]
  predicted label = 0.862838652838, expected label = 1
------------------------------------------------------------------------
Classifying line:
  603	It worked very well.	1
  words = [work, well, work well]
  predicted label = 0.861710903113, expected label = 1
------------------------------------------------------------------------
Classifying line:
  604	The company shipped my product very promptly and it works well.	1
  words = [compani, ship, product, promptli, work, well, compani ship, ship product, product promptli, promptli work, work well]
  predicted label = 0.541380113065, expected label = 1
------------------------------------------------------------------------
Classifying line:
  605	Exactly what I wanted.	1
  words = [exactli, want, exactli want]
  predicted label = 0.807255092199, expected label = 1
------------------------------------------------------------------------
Classifying line:
  606	This is a great deal.	1
  words = [great, deal, great deal]
  predicted label = 0.970948853702, expected label = 1
------------------------------------------------------------------------
Classifying line:
  607	Excellent product, I am very satisfied with the purchase.	1
  words = [excel, product, satisfi, purchas, excel product, product satisfi, satisfi purchas]
  predicted label = 0.916031814139, expected label = 1
------------------------------------------------------------------------
Classifying line:
  608	I highly recommend these and encourage people to give them a try.	1
  words = [highli, recommend, encourag, peopl, give, tri, highli recommend, recommend encourag, encourag peopl, peopl give, give tri]
  predicted label = 0.981853166602, expected label = 1
------------------------------------------------------------------------
Classifying line:
  609	Better than you'd expect.	1
  words = [better, expect, better expect]
  predicted label = 0.20023766616, expected label = 1
------------------------------------------------------------------------
Classifying line:
  610	Great product and price.	1
  words = [great, product, price, great product, product price]
  predicted label = 0.988734824237, expected label = 1

================================
You guessed 327/600 = 54.5% correct.
  - False positive rate: [0.0, 0.0025, 0.0025, 0.005, 0.005, 0.0075, 0.0075, 0.01, 0.01, 0.0125, 0.0125, 0.0125, 0.015, 0.015, 0.02, 0.02, 0.0225, 0.0225, 0.025, 0.025, 0.03, 0.03, 0.035, 0.035, 0.0375, 0.0375, 0.04, 0.04, 0.0425, 0.0425, 0.045, 0.045, 0.055, 0.055, 0.055, 0.0575, 0.0575, 0.065, 0.065, 0.07, 0.07, 0.0725, 0.0725, 0.0775, 0.0775, 0.0875, 0.0875, 0.09, 0.09, 0.0925, 0.0925, 0.0975, 0.0975, 0.1, 0.1, 0.1075, 0.1075, 0.1125, 0.1125, 0.115, 0.115, 0.1175, 0.1175, 0.12, 0.12, 0.1225, 0.1225, 0.125, 0.125, 0.1275, 0.1275, 0.1325, 0.1325, 0.135, 0.135, 0.14, 0.14, 0.1525, 0.1525, 0.155, 0.155, 0.1575, 0.1575, 0.1575, 0.16, 0.16, 0.1625, 0.1625, 0.17, 0.17, 0.1725, 0.1725, 0.1725, 0.175, 0.175, 0.1775, 0.1775, 0.185, 0.185, 0.1875, 0.1925, 0.205, 0.205, 0.2075, 0.2075, 0.2175, 0.2175, 0.2225, 0.225, 0.225, 0.25, 0.25, 0.255, 0.2575, 0.2675, 0.2675, 0.2725, 0.2725, 0.295, 0.295, 0.3125, 0.3125, 0.3175, 0.3175, 0.3225, 0.3275, 0.3275, 0.335, 0.3375, 0.3625, 0.3625, 0.365, 0.37, 0.3725, 0.3725, 0.38, 0.38, 0.3825, 0.3825, 0.3875, 0.39, 0.395, 0.4025, 0.4025, 0.415, 0.415, 0.43, 0.435, 0.44, 0.44, 0.445, 0.445, 0.4475, 0.4475, 0.45, 0.45, 0.4575, 0.4775, 0.4775, 0.485, 0.53, 0.535, 0.535, 0.535, 0.55, 0.55, 0.555, 0.555, 0.5725, 0.5725, 0.59, 0.59, 0.5925, 0.5975, 0.6125, 0.615, 0.6225, 0.6275, 0.6275, 0.63, 0.63, 0.6325, 0.6325, 0.635, 0.635, 0.64, 0.645, 0.645, 0.65, 0.65, 0.6575, 0.66, 0.66, 0.69, 0.69, 0.6975, 0.6975, 0.71, 0.7175, 0.7225, 0.73, 0.73, 0.735, 0.735, 0.7375, 0.7375, 0.7575, 0.7575, 0.765, 0.765, 0.7775, 0.7775, 0.7975, 0.7975, 0.805, 0.805, 0.8225, 0.8225, 0.8325, 0.8325, 0.8375, 0.8375, 0.8425, 0.8425, 0.845, 0.845, 0.85, 0.85, 0.8525, 0.8525, 0.8575, 0.86, 0.86, 0.8625, 0.8625, 0.8675, 0.875, 0.8775, 0.8775, 0.8825, 0.8825, 0.8975, 0.8975, 0.9025, 0.9025, 0.905, 0.905, 0.91, 0.9125, 0.9125, 0.93, 0.93, 0.96, 0.96, 0.965, 0.965, 1.0]
  - True positive rate: [0.0, 0.0, 0.005, 0.005, 0.025, 0.025, 0.03, 0.03, 0.035, 0.035, 0.055, 0.075, 0.075, 0.09, 0.09, 0.095, 0.095, 0.105, 0.105, 0.12, 0.12, 0.125, 0.125, 0.135, 0.135, 0.14, 0.14, 0.145, 0.145, 0.155, 0.155, 0.16, 0.16, 0.175, 0.18, 0.18, 0.19, 0.19, 0.195, 0.195, 0.21, 0.21, 0.215, 0.215, 0.235, 0.235, 0.24, 0.24, 0.25, 0.25, 0.255, 0.255, 0.26, 0.26, 0.27, 0.27, 0.28, 0.28, 0.29, 0.29, 0.3, 0.3, 0.305, 0.305, 0.31, 0.31, 0.325, 0.325, 0.34, 0.34, 0.345, 0.345, 0.35, 0.35, 0.365, 0.365, 0.38, 0.38, 0.385, 0.385, 0.39, 0.39, 0.395, 0.41, 0.41, 0.42, 0.42, 0.425, 0.425, 0.44, 0.44, 0.455, 0.465, 0.465, 0.47, 0.47, 0.475, 0.475, 0.48, 0.48, 0.48, 0.48, 0.49, 0.495, 0.5, 0.5, 0.505, 0.505, 0.505, 0.51, 0.51, 0.515, 0.515, 0.52, 0.52, 0.53, 0.53, 0.54, 0.54, 0.545, 0.545, 0.555, 0.555, 0.56, 0.565, 0.565, 0.575, 0.575, 0.58, 0.58, 0.59, 0.59, 0.59, 0.59, 0.595, 0.595, 0.605, 0.605, 0.61, 0.615, 0.615, 0.615, 0.615, 0.62, 0.62, 0.625, 0.625, 0.625, 0.625, 0.635, 0.635, 0.64, 0.64, 0.645, 0.645, 0.66, 0.66, 0.66, 0.665, 0.665, 0.7, 0.7, 0.715, 0.72, 0.72, 0.725, 0.725, 0.73, 0.73, 0.745, 0.745, 0.76, 0.76, 0.76, 0.76, 0.77, 0.77, 0.77, 0.775, 0.775, 0.78, 0.78, 0.785, 0.785, 0.79, 0.79, 0.79, 0.795, 0.795, 0.81, 0.81, 0.81, 0.815, 0.815, 0.835, 0.835, 0.845, 0.845, 0.845, 0.845, 0.845, 0.85, 0.85, 0.855, 0.855, 0.86, 0.86, 0.865, 0.865, 0.87, 0.87, 0.875, 0.875, 0.885, 0.885, 0.89, 0.89, 0.9, 0.9, 0.905, 0.905, 0.91, 0.91, 0.915, 0.915, 0.92, 0.92, 0.925, 0.925, 0.935, 0.935, 0.935, 0.94, 0.94, 0.945, 0.945, 0.945, 0.945, 0.95, 0.95, 0.955, 0.955, 0.96, 0.96, 0.965, 0.965, 0.97, 0.97, 0.97, 0.98, 0.98, 0.99, 0.99, 0.995, 0.995, 1.0, 1.0]
  - Thresholds: [1.9995749637509663, 0.9995749637509663, 0.9971946743700563, 0.996924711744282, 0.9956025212074827, 0.9951777637473678, 0.9921118630794863, 0.9911720666657022, 0.9905883165258332, 0.9899623554908358, 0.9887348242374302, 0.9876246350271869, 0.987558139167127, 0.9849854553935702, 0.9845825838945963, 0.984202165389252, 0.9836550515053618, 0.9802091992136961, 0.9799063730231902, 0.9781753062750019, 0.9764585480945199, 0.976271440467796, 0.9759964664325376, 0.9756239049389217, 0.9749569893739986, 0.9727473372215241, 0.9718153727533481, 0.9709488537024069, 0.9709196670450871, 0.9688200989365945, 0.9674519212397202, 0.9670525653218414, 0.9619682779587543, 0.9610475185931667, 0.9598767481181918, 0.9596759884557202, 0.9578082068820644, 0.9567463618317118, 0.9549696375278305, 0.9521883780477259, 0.9493648018969367, 0.948678914037719, 0.9478081662570128, 0.9468537577506507, 0.9453167064621071, 0.9442309668383866, 0.9415578182239953, 0.9412438692246154, 0.9380634417830184, 0.9376138265452262, 0.9354722732444604, 0.9344780975082113, 0.9327916188208367, 0.9326275294667323, 0.9281865562469619, 0.9233820256034478, 0.9189212943373359, 0.9169026187370688, 0.915564660246117, 0.9149751127002608, 0.9137763184185045, 0.9135071401119478, 0.9120054438765917, 0.9113866692420298, 0.9091655007255369, 0.9085753366356291, 0.907485649931856, 0.9073827539135518, 0.8932303215997045, 0.8923465296260011, 0.8906424832115798, 0.8878904859659403, 0.8864628013545534, 0.8837051617895435, 0.88198707617806, 0.8785458480384324, 0.8779090559142794, 0.8738024267791107, 0.8737086090712468, 0.8728103414131243, 0.8679512690135557, 0.8645190900263566, 0.8628386528377959, 0.8617109031129289, 0.8590127129629882, 0.8581687479163388, 0.8556275011863477, 0.8518896981618772, 0.8468905548288992, 0.8453265952253471, 0.8449241778136001, 0.8400340303437286, 0.8354025789882626, 0.835241021968725, 0.8301300390624369, 0.830086257477504, 0.8277839702049333, 0.8242317181996542, 0.8237933570486998, 0.8225635773750146, 0.8217132740654803, 0.8142622506674652, 0.8068072826976176, 0.8059816648435533, 0.8059642716363385, 0.7906451615058726, 0.7848909503745091, 0.7848396306362263, 0.781713730415289, 0.7813834738636539, 0.7573910891522877, 0.7553599210842519, 0.7549035749184143, 0.7546781062991582, 0.7469350537342512, 0.7453614464056777, 0.7405242278730628, 0.7372137434239309, 0.715382431422515, 0.7105548091668823, 0.6972461065512603, 0.6941075654193817, 0.6874504073241703, 0.686986894960021, 0.6869082949850667, 0.6726140806268636, 0.6720365871818866, 0.6705926604071369, 0.6681997371309274, 0.6416288354560539, 0.638725391466665, 0.6317484571264522, 0.6314060130219904, 0.6305810621727059, 0.6287031658025815, 0.6093364042062006, 0.6081691392859612, 0.6074382707296283, 0.606425167942742, 0.6059986732737094, 0.6059859115581161, 0.605575878981165, 0.6016199395192846, 0.5929097140896616, 0.5853986451603318, 0.5819187733922594, 0.569442543520594, 0.5680330466099122, 0.5650201823881933, 0.5617884556594186, 0.5519911682892564, 0.550996649530135, 0.5486222926017286, 0.5419562157154463, 0.5418541802326394, 0.539684509973335, 0.5271092241453242, 0.5204605952681396, 0.5196295583114402, 0.5087606793311276, 0.5085385837896609, 0.4961155051000052, 0.49190157068303814, 0.49080889100156333, 0.4786972904937058, 0.4769033616479704, 0.47610256382823235, 0.476040987859884, 0.4604824368920628, 0.44980884685479455, 0.4331895674105994, 0.4279318695403008, 0.42253535457032004, 0.42151661798952955, 0.4046401315208045, 0.4045020219311834, 0.39612705569810897, 0.39248525313567445, 0.388589298517042, 0.38318886120019524, 0.3780818116103632, 0.3771779492611175, 0.37101237054854325, 0.37096384150933626, 0.36793193481246, 0.3660558117651572, 0.3641489577753081, 0.3640416302993132, 0.3596844192302141, 0.3462220319173581, 0.3439629034344761, 0.34035660232715526, 0.33940363554203684, 0.3181680399240342, 0.31115135893597773, 0.30248044519364686, 0.3003277848386231, 0.29502483276274394, 0.2937434326872538, 0.2934677605552953, 0.29158846903196906, 0.2885005550740017, 0.2858996212713409, 0.28348115441574306, 0.28073068080119234, 0.2759199245358489, 0.25575050791824977, 0.25319906479577264, 0.24645160910502742, 0.24582256178085915, 0.23151977240567312, 0.2261395925094898, 0.2042862745277221, 0.20023766616026736, 0.18314923319700527, 0.1831040689094203, 0.1557571560736112, 0.15342867955476386, 0.14505988049900945, 0.14213645644255934, 0.13455930128850618, 0.13452329057762333, 0.12833040108336866, 0.12823355389864988, 0.12790133089452818, 0.12120346190532336, 0.11862060655785363, 0.11798374854498449, 0.11572029338729926, 0.11088854875503808, 0.11009455684522983, 0.1094509007597365, 0.10869450796059374, 0.10776911005168212, 0.1063844074834625, 0.10559347051851421, 0.10553246593951171, 0.10465112597057741, 0.10458695539492159, 0.10355563259166142, 0.10315743781473635, 0.09022547578521296, 0.08693259964388468, 0.0866839548429682, 0.08658857378371906, 0.08484152349592444, 0.08155063513360761, 0.07991152422067892, 0.07795012291867844, 0.07125614073230666, 0.05499765253742989, 0.047109847320582494, 0.02566587472021187, 0.02332671472722437, 0.021537726270483466, 0.020548572022524077, 0.0002729036758740172]
  - AUC: 0.65751875
  - F1: 0.506329113924
================================

Running model 'mnb' on train data 'default.txt2.train' and test data 'default.txt2.test'
------------------------------------------------------------------------
Classifying line:
  601	Magical Help.	1
  words = [magic, help, magic help]
  predicted label = 0.573028977283, expected label = 1
------------------------------------------------------------------------
Classifying line:
  602	The best phone in market :).	1
  words = [best, phone, market, best phone, phone market]
  predicted label = 0.881864361606, expected label = 1
------------------------------------------------------------------------
Classifying line:
  603	It worked very well.	1
  words = [work, well, work well]
  predicted label = 0.874683249882, expected label = 1
------------------------------------------------------------------------
Classifying line:
  604	The company shipped my product very promptly and it works well.	1
  words = [compani, ship, product, promptli, work, well, compani ship, ship product, product promptli, promptli work, work well]
  predicted label = 0.550489601931, expected label = 1
------------------------------------------------------------------------
Classifying line:
  605	Exactly what I wanted.	1
  words = [exactli, want, exactli want]
  predicted label = 0.805717226475, expected label = 1
------------------------------------------------------------------------
Classifying line:
  606	This is a great deal.	1
  words = [great, deal, great deal]
  predicted label = 0.970960867072, expected label = 1
------------------------------------------------------------------------
Classifying line:
  607	Excellent product, I am very satisfied with the purchase.	1
  words = [excel, product, satisfi, purchas, excel product, product satisfi, satisfi purchas]
  predicted label = 0.921992861997, expected label = 1
------------------------------------------------------------------------
Classifying line:
  608	I highly recommend these and encourage people to give them a try.	1
  words = [highli, recommend, encourag, peopl, give, tri, highli recommend, recommend encourag, encourag peopl, peopl give, give tri]
  predicted label = 0.980317027353, expected label = 1
------------------------------------------------------------------------
Classifying line:
  609	Better than you'd expect.	1
  words = [better, expect, better expect]
  predicted label = 0.230219939687, expected label = 1
------------------------------------------------------------------------
Classifying line:
  610	Great product and price.	1
  words = [great, product, price, great product, product price]
  predicted label = 0.98819222256, expected label = 1

================================
You guessed 333/600 = 55.5% correct.
  - False positive rate: [0.0, 0.0025, 0.0025, 0.0075, 0.0075, 0.01, 0.01, 0.0125, 0.0125, 0.0175, 0.0175, 0.0175, 0.02, 0.02, 0.025, 0.025, 0.03, 0.03, 0.035, 0.035, 0.035, 0.035, 0.0375, 0.0375, 0.04, 0.04, 0.0475, 0.0475, 0.055, 0.055, 0.0575, 0.0575, 0.06, 0.06, 0.0725, 0.0725, 0.075, 0.075, 0.0775, 0.0775, 0.08, 0.08, 0.085, 0.085, 0.0875, 0.0875, 0.0925, 0.0925, 0.095, 0.095, 0.1, 0.1, 0.1025, 0.1025, 0.1075, 0.1075, 0.11, 0.11, 0.1125, 0.1125, 0.115, 0.115, 0.12, 0.12, 0.1225, 0.1225, 0.1275, 0.1275, 0.13, 0.13, 0.1325, 0.1325, 0.145, 0.145, 0.15, 0.15, 0.1525, 0.1525, 0.1525, 0.165, 0.165, 0.1675, 0.1675, 0.17, 0.17, 0.1725, 0.1725, 0.1775, 0.1775, 0.1775, 0.185, 0.19, 0.19, 0.1925, 0.1925, 0.205, 0.2075, 0.2175, 0.2175, 0.2225, 0.23, 0.23, 0.235, 0.235, 0.245, 0.245, 0.2475, 0.25, 0.255, 0.255, 0.2575, 0.2575, 0.26, 0.26, 0.285, 0.285, 0.2975, 0.3025, 0.305, 0.305, 0.31, 0.315, 0.315, 0.3225, 0.325, 0.335, 0.335, 0.34, 0.34, 0.35, 0.35, 0.355, 0.36, 0.3675, 0.3675, 0.3675, 0.3675, 0.375, 0.385, 0.3875, 0.3875, 0.3975, 0.4025, 0.4025, 0.41, 0.415, 0.415, 0.4175, 0.4175, 0.42, 0.42, 0.43, 0.4375, 0.4425, 0.4525, 0.4975, 0.5, 0.5, 0.5175, 0.5175, 0.52, 0.52, 0.525, 0.5275, 0.5475, 0.5475, 0.55, 0.55, 0.5575, 0.5575, 0.565, 0.565, 0.5775, 0.5825, 0.5875, 0.5875, 0.59, 0.5925, 0.6, 0.6, 0.62, 0.62, 0.625, 0.625, 0.6275, 0.635, 0.64, 0.64, 0.65, 0.65, 0.655, 0.655, 0.6725, 0.6725, 0.68, 0.69, 0.695, 0.7, 0.7, 0.7025, 0.7025, 0.7075, 0.715, 0.715, 0.73, 0.73, 0.755, 0.755, 0.765, 0.765, 0.7675, 0.7675, 0.7675, 0.7725, 0.7725, 0.7875, 0.7875, 0.79, 0.79, 0.8, 0.8, 0.805, 0.805, 0.825, 0.825, 0.8275, 0.8275, 0.83, 0.83, 0.8325, 0.8325, 0.85, 0.85, 0.8575, 0.865, 0.865, 0.8675, 0.8675, 0.87, 0.87, 0.8725, 0.8725, 0.875, 0.88, 0.8825, 0.8825, 0.8925, 0.8925, 0.9175, 0.9175, 0.92, 0.92, 0.9225, 0.9225, 0.9375, 0.9375, 0.945, 0.945, 0.955, 0.955, 1.0]
  - True positive rate: [0.005, 0.005, 0.015, 0.015, 0.02, 0.02, 0.03, 0.03, 0.035, 0.035, 0.055, 0.075, 0.075, 0.09, 0.09, 0.095, 0.095, 0.11, 0.11, 0.115, 0.125, 0.135, 0.135, 0.14, 0.14, 0.155, 0.155, 0.17, 0.17, 0.175, 0.175, 0.18, 0.18, 0.19, 0.19, 0.195, 0.195, 0.2, 0.2, 0.22, 0.22, 0.23, 0.23, 0.235, 0.235, 0.24, 0.24, 0.255, 0.255, 0.26, 0.26, 0.265, 0.265, 0.275, 0.275, 0.285, 0.285, 0.295, 0.295, 0.3, 0.3, 0.305, 0.305, 0.325, 0.325, 0.335, 0.335, 0.34, 0.34, 0.355, 0.355, 0.37, 0.37, 0.375, 0.375, 0.39, 0.39, 0.4, 0.415, 0.415, 0.42, 0.42, 0.43, 0.43, 0.435, 0.435, 0.445, 0.445, 0.455, 0.465, 0.465, 0.465, 0.47, 0.47, 0.475, 0.475, 0.48, 0.48, 0.49, 0.49, 0.49, 0.495, 0.495, 0.5, 0.5, 0.505, 0.505, 0.51, 0.51, 0.52, 0.52, 0.53, 0.53, 0.535, 0.535, 0.54, 0.54, 0.54, 0.54, 0.55, 0.55, 0.555, 0.565, 0.565, 0.57, 0.57, 0.575, 0.575, 0.58, 0.58, 0.585, 0.585, 0.585, 0.585, 0.59, 0.6, 0.61, 0.61, 0.615, 0.615, 0.62, 0.62, 0.62, 0.625, 0.625, 0.625, 0.635, 0.635, 0.645, 0.645, 0.655, 0.655, 0.655, 0.655, 0.655, 0.69, 0.69, 0.705, 0.705, 0.715, 0.715, 0.72, 0.72, 0.725, 0.725, 0.74, 0.74, 0.745, 0.745, 0.75, 0.75, 0.755, 0.755, 0.755, 0.755, 0.765, 0.77, 0.77, 0.77, 0.775, 0.775, 0.78, 0.78, 0.785, 0.785, 0.785, 0.785, 0.8, 0.8, 0.805, 0.805, 0.81, 0.81, 0.815, 0.815, 0.815, 0.815, 0.815, 0.83, 0.83, 0.84, 0.84, 0.84, 0.845, 0.845, 0.85, 0.85, 0.86, 0.86, 0.865, 0.865, 0.875, 0.885, 0.885, 0.89, 0.89, 0.895, 0.895, 0.9, 0.9, 0.905, 0.905, 0.91, 0.91, 0.92, 0.92, 0.925, 0.925, 0.93, 0.93, 0.935, 0.935, 0.94, 0.94, 0.94, 0.945, 0.945, 0.95, 0.95, 0.955, 0.955, 0.96, 0.96, 0.96, 0.96, 0.965, 0.965, 0.97, 0.97, 0.975, 0.975, 0.98, 0.98, 0.985, 0.985, 0.99, 0.99, 0.995, 0.995, 1.0, 1.0]
  - Thresholds: [0.9997627695787004, 0.9992807583795925, 0.9967693610197067, 0.9965808188009009, 0.9964771679032353, 0.9964522727690014, 0.9951769530444675, 0.9941755812322761, 0.9921211681239294, 0.9895389924901379, 0.9881922225603915, 0.985652052512555, 0.9855233162860845, 0.9833606743762868, 0.9824813049414428, 0.9815281870341743, 0.9806717068759893, 0.978416669556499, 0.9770093714286193, 0.9761281298003093, 0.9756830319485936, 0.9747058913960102, 0.9741268307189183, 0.9735984416666249, 0.9734469482335354, 0.9709608670718421, 0.9662677446051068, 0.9652510413995709, 0.9631538953922663, 0.96253344967341, 0.9613615417710334, 0.9610179787913946, 0.9606582466514146, 0.9575966229552851, 0.9537688385436538, 0.9530680112305224, 0.9527705694632054, 0.9519224826339983, 0.9498856530030321, 0.9478893458997647, 0.9464958246166121, 0.9453845962554499, 0.943695047625731, 0.9431420453738311, 0.9428564432293964, 0.9421063264206955, 0.9410602428364856, 0.9362961085947806, 0.9353183084730682, 0.9339838447084322, 0.9333940616064198, 0.9329810433239721, 0.9328863888129594, 0.924878747761486, 0.9242059058945304, 0.9238836012695965, 0.9231156691256054, 0.9200226302525334, 0.9169087973687703, 0.9162191688334687, 0.9159645605472491, 0.9151287681405491, 0.9112155629789009, 0.9054844921028978, 0.9035947540693462, 0.8986264365148522, 0.8979728931734166, 0.8972551265381348, 0.8961325228775637, 0.8910986163353046, 0.8909335478467664, 0.8898294420491304, 0.8837626136160893, 0.8818643616061225, 0.880342413123495, 0.8779948500093454, 0.8772704344601188, 0.8747852027962333, 0.8746832498823213, 0.8641068273205343, 0.8583394504914711, 0.8580902260213549, 0.856326395813986, 0.854010050871816, 0.8518906562529761, 0.8508926442610414, 0.8449173175767357, 0.8410996257483409, 0.8409567870026199, 0.8363723490222059, 0.8318844051531585, 0.8306458727805677, 0.8302253048563012, 0.8299025931062135, 0.8293493767203729, 0.823033261585722, 0.8224863329063028, 0.8061240213074097, 0.8057172264745133, 0.8039522406458813, 0.794006027394507, 0.7932413095825397, 0.7931539041369222, 0.7914465498573336, 0.7760163886443553, 0.7728278532177453, 0.7704769694270163, 0.7703893291332384, 0.7657429406768311, 0.7610825592248548, 0.7607037408694859, 0.7541257025061958, 0.749871995589616, 0.7475424058946316, 0.7322598862774665, 0.7318061832173641, 0.7164859008301588, 0.7161471914549672, 0.71318466263302, 0.7111052851750808, 0.7064711739071721, 0.7055853581571476, 0.6995877310555118, 0.6910527826221331, 0.6866333355724666, 0.6673516333508857, 0.6635881316868667, 0.6603226651580457, 0.6592071261732851, 0.6542441758578099, 0.6533870163971054, 0.6516460877356983, 0.650024382009828, 0.6478442919413526, 0.6465357789240419, 0.6403277857937464, 0.6373506776191343, 0.6327116949371944, 0.6265311023937236, 0.6160560668519617, 0.6139472872749038, 0.5949412360959524, 0.5898179899762596, 0.5837893218027788, 0.5759219417593409, 0.5743012184110008, 0.5642321593602089, 0.5628906533958367, 0.5611560422373701, 0.5582220727404339, 0.5504896019305275, 0.5460298032781183, 0.5450999512432962, 0.5450537253048527, 0.5427260463878789, 0.5416666666666667, 0.5384009353043212, 0.5366805793773708, 0.5217807993895357, 0.5184576385510311, 0.5180631558126784, 0.517680747176562, 0.5061605260108565, 0.5057438805053708, 0.4839967569648751, 0.4766306908699264, 0.47348604888663737, 0.4709427603972588, 0.46693407541328036, 0.46639438571938036, 0.4631637091762759, 0.4600763879065345, 0.4451452490930233, 0.4440913604766636, 0.4342258049752387, 0.428479863423903, 0.42713094734544077, 0.423408257770798, 0.41825664047886313, 0.41785112888284953, 0.40489991842001166, 0.4041183577642298, 0.38412791387456885, 0.3835039882902209, 0.38334069249751884, 0.38007499243190557, 0.37686580919539986, 0.36575383579904946, 0.35246232833369784, 0.3499445254092991, 0.34750174838759823, 0.34680332689238047, 0.3304787914917815, 0.3264617186680646, 0.32401101289668205, 0.3094948916613779, 0.3083843122640829, 0.30642803133821633, 0.30431694024025496, 0.30429619987956213, 0.30250769442780073, 0.29547685569734955, 0.29368238425591253, 0.29252490464115244, 0.2754520642496109, 0.2723584257409475, 0.2498612768262345, 0.24637430955082493, 0.23311788907458192, 0.23284238513009967, 0.2321840124682391, 0.23021993968668328, 0.22819169499836645, 0.2204305341184931, 0.21728597121760987, 0.19332526370396017, 0.1910254896722103, 0.19070157512685992, 0.18757645848783808, 0.17571696901446612, 0.17184263983796352, 0.16158415386485425, 0.15671768747418663, 0.13479199465971806, 0.13262676325654627, 0.13259761824238403, 0.1266616364004823, 0.12045142349233721, 0.11824247579633526, 0.11742950791012131, 0.11739744655695686, 0.10776287191145621, 0.10757282344218484, 0.10700612557427265, 0.09784768007012902, 0.09757598746684952, 0.09707539792335089, 0.09624182914164689, 0.09458527375655126, 0.09409880118070157, 0.08893310183269312, 0.08887250456408408, 0.08748483640804072, 0.08747701576620635, 0.08703882658731808, 0.08612873181804528, 0.06981527710220674, 0.0688104559318061, 0.05166263285024939, 0.04903049514302647, 0.04546805203387232, 0.04415724490274339, 0.0419211536188855, 0.03775316565309423, 0.029534713990941135, 0.028538417470642983, 0.025552189341779774, 0.022949388490293286, 0.02008722239042892, 0.019784191110765206, 0.00013718623959872248]
  - AUC: 0.66149375
  - F1: 0.520646319569
================================

Running model 'dt' on train data 'default.txt2.train' and test data 'default.txt2.test'
Best hyperparameters were: {'max_features': 0.5, 'min_samples_leaf': 1}
------------------------------------------------------------------------
Classifying line:
  601	Magical Help.	1
  words = [magic, help, magic help]
  predicted label = 0.0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  602	The best phone in market :).	1
  words = [best, phone, market, best phone, phone market]
  predicted label = 1.0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  603	It worked very well.	1
  words = [work, well, work well]
  predicted label = 1.0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  604	The company shipped my product very promptly and it works well.	1
  words = [compani, ship, product, promptli, work, well, compani ship, ship product, product promptli, promptli work, work well]
  predicted label = 1.0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  605	Exactly what I wanted.	1
  words = [exactli, want, exactli want]
  predicted label = 1.0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  606	This is a great deal.	1
  words = [great, deal, great deal]
  predicted label = 1.0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  607	Excellent product, I am very satisfied with the purchase.	1
  words = [excel, product, satisfi, purchas, excel product, product satisfi, satisfi purchas]
  predicted label = 1.0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  608	I highly recommend these and encourage people to give them a try.	1
  words = [highli, recommend, encourag, peopl, give, tri, highli recommend, recommend encourag, encourag peopl, peopl give, give tri]
  predicted label = 1.0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  609	Better than you'd expect.	1
  words = [better, expect, better expect]
  predicted label = 1.0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  610	Great product and price.	1
  words = [great, product, price, great product, product price]
  predicted label = 1.0, expected label = 1

================================
You guessed 329/600 = 54.8333333333% correct.
  - False positive rate: [0.0, 0.4375, 0.4375, 0.445, 0.4575, 0.465, 0.47, 0.54, 0.6075, 0.61, 0.6125, 0.675, 1.0]
  - True positive rate: [0.0, 0.595, 0.605, 0.615, 0.645, 0.67, 0.67, 0.725, 0.77, 0.775, 0.775, 0.775, 1.0]
  - Thresholds: [2.0, 1.0, 0.8571428571428571, 0.8181818181818182, 0.7222222222222222, 0.6666666666666666, 0.6, 0.5, 0.4482758620689655, 0.3333333333333333, 0.25, 0.16666666666666666, 0.0]
  - AUC: 0.5909125
  - F1: 0.516934046346
================================

Running model 'rf' on train data 'default.txt2.train' and test data 'default.txt2.test'
Best hyperparameters were: {'n_estimators': 30}
------------------------------------------------------------------------
Classifying line:
  601	Magical Help.	1
  words = [magic, help, magic help]
  predicted label = 0.481248127156, expected label = 1
------------------------------------------------------------------------
Classifying line:
  602	The best phone in market :).	1
  words = [best, phone, market, best phone, phone market]
  predicted label = 0.958742121868, expected label = 1
------------------------------------------------------------------------
Classifying line:
  603	It worked very well.	1
  words = [work, well, work well]
  predicted label = 0.778085857913, expected label = 1
------------------------------------------------------------------------
Classifying line:
  604	The company shipped my product very promptly and it works well.	1
  words = [compani, ship, product, promptli, work, well, compani ship, ship product, product promptli, promptli work, work well]
  predicted label = 0.765769352363, expected label = 1
------------------------------------------------------------------------
Classifying line:
  605	Exactly what I wanted.	1
  words = [exactli, want, exactli want]
  predicted label = 0.481248127156, expected label = 1
------------------------------------------------------------------------
Classifying line:
  606	This is a great deal.	1
  words = [great, deal, great deal]
  predicted label = 0.990784006472, expected label = 1
------------------------------------------------------------------------
Classifying line:
  607	Excellent product, I am very satisfied with the purchase.	1
  words = [excel, product, satisfi, purchas, excel product, product satisfi, satisfi purchas]
  predicted label = 0.965740081981, expected label = 1
------------------------------------------------------------------------
Classifying line:
  608	I highly recommend these and encourage people to give them a try.	1
  words = [highli, recommend, encourag, peopl, give, tri, highli recommend, recommend encourag, encourag peopl, peopl give, give tri]
  predicted label = 0.571305432456, expected label = 1
------------------------------------------------------------------------
Classifying line:
  609	Better than you'd expect.	1
  words = [better, expect, better expect]
  predicted label = 0.396645435057, expected label = 1
------------------------------------------------------------------------
Classifying line:
  610	Great product and price.	1
  words = [great, product, price, great product, product price]
  predicted label = 0.990784006472, expected label = 1

================================
You guessed 316/600 = 52.6666666667% correct.
  - False positive rate: [0.0, 0.01, 0.01, 0.01, 0.01, 0.015, 0.0275, 0.03, 0.03, 0.03, 0.03, 0.0375, 0.0375, 0.0475, 0.0625, 0.07, 0.0825, 0.1725, 0.1775, 0.1825, 0.1825, 0.19, 0.195, 0.195, 0.1975, 0.2, 0.205, 0.205, 0.2225, 0.225, 0.225, 0.2275, 0.2325, 0.2425, 0.25, 0.2525, 0.2525, 0.3125, 0.3225, 0.3275, 0.3275, 0.33, 0.34, 0.34, 0.345, 0.3475, 0.3525, 0.3625, 0.3625, 0.365, 0.365, 0.365, 0.385, 0.3925, 0.3925, 0.395, 0.395, 0.4, 0.4025, 0.4025, 0.41, 0.42, 0.42, 0.4275, 0.4275, 0.4325, 0.4375, 0.4375, 0.4475, 0.4475, 0.4575, 0.4575, 0.46, 0.46, 0.48, 0.4825, 0.4875, 0.49, 0.49, 0.4925, 0.4925, 0.4975, 0.5, 0.505, 0.5125, 0.5175, 0.52, 0.735, 0.7375, 0.7425, 0.7425, 0.745, 0.7475, 0.7475, 0.75, 0.755, 0.755, 0.7575, 0.7575, 0.7625, 0.765, 0.765, 0.7675, 0.7675, 0.7725, 0.7725, 0.7825, 0.79, 0.79, 0.795, 0.7975, 0.805, 0.81, 0.8125, 0.815, 0.815, 0.8175, 0.8225, 0.8225, 0.825, 0.8275, 0.8275, 0.8275, 0.8325, 0.8325, 0.8375, 0.8375, 0.8475, 0.8475, 0.8525, 0.8525, 0.8575, 0.8575, 0.8625, 0.87, 0.87, 0.875, 0.875, 0.88, 0.88, 0.885, 0.885, 0.8875, 0.89, 0.89, 0.8925, 0.8925, 0.895, 0.895, 0.895, 0.8975, 0.8975, 0.9075, 0.9125, 0.915, 0.9475, 0.96, 0.96, 0.9825, 0.99, 0.9975, 1.0]
  - True positive rate: [0.0, 0.16, 0.165, 0.195, 0.2, 0.225, 0.265, 0.265, 0.275, 0.285, 0.29, 0.29, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.305, 0.305, 0.305, 0.32, 0.32, 0.33, 0.33, 0.34, 0.41, 0.41, 0.415, 0.415, 0.415, 0.415, 0.415, 0.415, 0.42, 0.42, 0.42, 0.42, 0.43, 0.44, 0.44, 0.445, 0.445, 0.445, 0.445, 0.445, 0.455, 0.455, 0.465, 0.47, 0.47, 0.47, 0.48, 0.48, 0.485, 0.5, 0.5, 0.505, 0.505, 0.505, 0.515, 0.515, 0.52, 0.52, 0.52, 0.525, 0.525, 0.53, 0.53, 0.54, 0.545, 0.55, 0.55, 0.55, 0.555, 0.555, 0.56, 0.56, 0.565, 0.575, 0.58, 0.58, 0.58, 0.58, 0.58, 0.7, 0.7, 0.705, 0.71, 0.71, 0.72, 0.725, 0.725, 0.725, 0.73, 0.73, 0.74, 0.74, 0.74, 0.745, 0.745, 0.755, 0.755, 0.76, 0.76, 0.76, 0.77, 0.77, 0.775, 0.775, 0.775, 0.78, 0.78, 0.785, 0.785, 0.785, 0.79, 0.795, 0.795, 0.8, 0.81, 0.81, 0.82, 0.82, 0.825, 0.825, 0.83, 0.83, 0.84, 0.84, 0.845, 0.845, 0.845, 0.85, 0.85, 0.855, 0.855, 0.875, 0.875, 0.88, 0.9, 0.9, 0.905, 0.905, 0.935, 0.935, 0.985, 0.99, 0.99, 0.995, 0.995, 0.995, 0.995, 0.995, 0.995, 1.0, 1.0, 1.0, 1.0, 1.0]
  - Thresholds: [1.9907840064718325, 0.9907840064718324, 0.9772127409285781, 0.9657400819812652, 0.9645660197202465, 0.9627070676185333, 0.9587421218677606, 0.9574506731384991, 0.9495123419334365, 0.9486071109130604, 0.9340734153145985, 0.8770735016051903, 0.8580611544164534, 0.8463176653144857, 0.8459518138900155, 0.8386479879865089, 0.8386436726404931, 0.8343578645297188, 0.8166376346997445, 0.8143122616880222, 0.812430937107301, 0.7903803955499925, 0.7822284187600947, 0.7780858579125247, 0.7752963762924572, 0.7711588640985553, 0.7678047046506458, 0.7657693523629365, 0.7617109546506458, 0.7612584906810699, 0.7609951340898393, 0.7593370749473748, 0.7592189747576956, 0.7448031429174627, 0.742509202085316, 0.7423963106193262, 0.7403896333293243, 0.7401023697871794, 0.7233797120790794, 0.7220468142316236, 0.7204876184793783, 0.7171922823644916, 0.704304822359229, 0.7036356293715299, 0.6948916418178308, 0.691935276863053, 0.6911553289146244, 0.6749968710659517, 0.6685816338797017, 0.6639617162689879, 0.6623028507496512, 0.6611078375747945, 0.6254700757949366, 0.6199596385400491, 0.618411607935177, 0.614873337902066, 0.5985387803146031, 0.5953205834513132, 0.5948327068716235, 0.5931839177575885, 0.5785483337863259, 0.5762543929541789, 0.5684843811569829, 0.5657203493584929, 0.5632118765134859, 0.5582261567941759, 0.5544418063958686, 0.5399028080074297, 0.5295894019966089, 0.5291724859439058, 0.5248236031159208, 0.5205137057441293, 0.5202846689023118, 0.5168588111643689, 0.5133030544513073, 0.5100962822340002, 0.5099848934385015, 0.5093965451206656, 0.5080272066768301, 0.5056985193332626, 0.50306731188862, 0.49558883390983516, 0.4946113084019713, 0.4911240576774923, 0.4856340920686811, 0.48411207986125404, 0.48172626017313863, 0.48124812715640036, 0.4808301657175351, 0.4785333617917085, 0.47124133936679885, 0.4706891845058511, 0.4702279606555736, 0.4679123213791231, 0.4670242709472501, 0.4646034772224506, 0.4618538701061782, 0.4611973951696628, 0.4554621836824329, 0.4513103405954622, 0.4510832512414956, 0.4444863253464226, 0.4419678962651593, 0.44118576153973077, 0.43930915485730915, 0.43696377016541377, 0.43483196513154726, 0.4315738319679836, 0.421650728944628, 0.4177060411175188, 0.4175239000271523, 0.4170664506174474, 0.41101407979421983, 0.41086929126606303, 0.4088287339287679, 0.40879782143339416, 0.40638246527389527, 0.404767032755594, 0.40412513799324007, 0.40113814927312796, 0.400583361939089, 0.3977776549325673, 0.39664543505691274, 0.3926561763436069, 0.38906371883246843, 0.38881309226605243, 0.3737843874397229, 0.3621751607396763, 0.36141650977351786, 0.3579448872834721, 0.34971238597181326, 0.3488577555326893, 0.343329750265459, 0.3431017119253956, 0.33464630300593257, 0.3333795522525783, 0.33120024350588206, 0.3247943444071803, 0.3147552520282655, 0.3080826266798775, 0.30674645178844256, 0.3043836385916733, 0.28863794274359184, 0.2872766670812204, 0.2776843771749953, 0.2628506919105377, 0.23759319915992558, 0.23756768580635618, 0.23190604292178077, 0.22956824413008128, 0.2287526253685232, 0.22774323071086497, 0.14713329222608318, 0.1366357336888768, 0.13243930511744822, 0.12317609333834227, 0.09340703452125854, 0.08521131381257047, 0.0746266379630842, 0.027210884353741496, 0.000980392156862745, 0.0]
  - AUC: 0.57628125
  - F1: 0.443137254902
================================

