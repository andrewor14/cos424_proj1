preprocessSentences.py:68: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal
  return [clean_word(w) for w in tokens if w.lower() not in stop_words and w.isalpha()]
Path: .
Training data: ./default.txt2.train
Done building things up man. Num documents in train set: 2400.
Number of features before any feature selection: 10596
Number of features after filtering out words by count threshold: 514
Done doing the feature selection thing man. Num vocabs: 462.
Output files: ./out*
Runtime: 3.91958999634
Running model 'mybnb' on train data 'default.txt2.train' and test data 'default.txt2.test'
Num train examples = 2400, positive prior = 0.541666666667, negative prior = 0.458333333333
Using 462 features: unit, music, hold, want, hot, wrong, beauti, fit, funni, silent...
442 features are used in positive reviews: forget, lack, month, abil, go, tv, friendli, send, certainli, worth...
425 features are used in negative reviews: forget, lack, month, abil, go, hate, tv, friendli, send, certainli...
------------------------------------------------------------------------
Classifying line:
  601	Magical Help.	1
  words = [magic, help, magic help]
  positive probability = 1.71789730317e-09, negative probability = 1.71240569786e-09
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  602	The best phone in market :).	1
  words = [best, phone, market, best phone, phone market]
  positive probability = 2.31631261326e-13, negative probability = 5.33009899333e-14
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  603	It worked very well.	1
  words = [work, well, work well]
  positive probability = 3.36707871422e-07, negative probability = 5.75368314479e-08
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  604	The company shipped my product very promptly and it works well.	1
  words = [compani, ship, product, promptli, work, well, compani ship, ship product, product promptli, promptli work, work well]
  positive probability = 9.78535615368e-31, negative probability = 2.3808700668e-30
  predicted label = 0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  605	Exactly what I wanted.	1
  words = [exactli, want, exactli want]
  positive probability = 2.35597344435e-08, negative probability = 5.82217937271e-09
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  606	This is a great deal.	1
  words = [great, deal, great deal]
  positive probability = 1.20743639023e-07, negative probability = 4.10977367485e-09
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  607	Excellent product, I am very satisfied with the purchase.	1
  words = [excel, product, satisfi, purchas, excel product, product satisfi, satisfi purchas]
  positive probability = 3.68925749102e-20, negative probability = 6.96677101902e-21
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  608	I highly recommend these and encourage people to give them a try.	1
  words = [highli, recommend, encourag, peopl, give, tri, highli recommend, recommend encourag, encourag peopl, peopl give, give tri]
  positive probability = 4.7010897899e-28, negative probability = 1.79585627896e-29
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  609	Better than you'd expect.	1
  words = [better, expect, better expect]
  positive probability = 1.96331120363e-08, negative probability = 8.01405866596e-08
  predicted label = 0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  610	Great product and price.	1
  words = [great, product, price, great product, product price]
  positive probability = 3.84623709432e-12, negative probability = 5.92233221481e-14
  predicted label = 1, expected label = 1

================================
You guessed 396/600 = 66.0% correct.
  - False positive rate: [0.0, 0.2875, 1.0]
  - True positive rate: [0.0, 0.555, 1.0]
  - Thresholds: [2, 1, 0]
  - AUC: 0.63375
  - F1: 0.521126760563
================================

Running model 'mymnb' on train data 'default.txt2.train' and test data 'default.txt2.test'
Num train examples = 2400, positive prior = 0.541666666667, negative prior = 0.458333333333
Using 462 features: unit, music, hold, want, hot, wrong, beauti, fit, funni, silent...
442 features are used in positive reviews: forget, lack, month, abil, go, tv, friendli, send, certainli, worth...
425 features are used in negative reviews: forget, lack, month, abil, go, hate, tv, friendli, send, certainli...
------------------------------------------------------------------------
Classifying line:
  601	Magical Help.	1
  words = [magic, help, magic help]
  positive probability = 6.93126396305e-10, negative probability = 6.01323752762e-10
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  602	The best phone in market :).	1
  words = [best, phone, market, best phone, phone market]
  positive probability = 5.97046881972e-14, negative probability = 9.61193681142e-15
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  603	It worked very well.	1
  words = [work, well, work well]
  positive probability = 1.49319229375e-07, negative probability = 2.02044780928e-08
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  604	The company shipped my product very promptly and it works well.	1
  words = [compani, ship, product, promptli, work, well, compani ship, ship product, product promptli, promptli work, work well]
  positive probability = 3.85726741954e-32, negative probability = 5.13147136371e-32
  predicted label = 0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  605	Exactly what I wanted.	1
  words = [exactli, want, exactli want]
  positive probability = 9.50573343504e-09, negative probability = 2.16476550994e-09
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  606	This is a great deal.	1
  words = [great, deal, great deal]
  positive probability = 5.10933172133e-08, negative probability = 1.44317700663e-09
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  607	Excellent product, I am very satisfied with the purchase.	1
  words = [excel, product, satisfi, purchas, excel product, product satisfi, satisfi purchas]
  positive probability = 4.68441782345e-21, negative probability = 6.06087729154e-22
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  608	I highly recommend these and encourage people to give them a try.	1
  words = [highli, recommend, encourag, peopl, give, tri, highli recommend, recommend encourag, encourag peopl, peopl give, give tri]
  positive probability = 1.79838698762e-29, negative probability = 3.87059554291e-31
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  609	Better than you'd expect.	1
  words = [better, expect, better expect]
  positive probability = 8.91162509535e-09, negative probability = 2.81419516293e-08
  predicted label = 0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  610	Great product and price.	1
  words = [great, product, price, great product, product price]
  positive probability = 8.88681320473e-13, negative probability = 1.03513165661e-14
  predicted label = 1, expected label = 1

================================
You guessed 386/600 = 64.3333333333% correct.
  - False positive rate: [0.0, 0.355, 1.0]
  - True positive rate: [0.0, 0.64, 1.0]
  - Thresholds: [2, 1, 0]
  - AUC: 0.6425
  - F1: 0.544680851064
================================

Running model 'bnb' on train data 'default.txt2.train' and test data 'default.txt2.test'
------------------------------------------------------------------------
Classifying line:
  601	Magical Help.	1
  words = [magic, help, magic help]
  predicted label = 0.549366857803, expected label = 1
------------------------------------------------------------------------
Classifying line:
  602	The best phone in market :).	1
  words = [best, phone, market, best phone, phone market]
  predicted label = 0.862057406489, expected label = 1
------------------------------------------------------------------------
Classifying line:
  603	It worked very well.	1
  words = [work, well, work well]
  predicted label = 0.860924268663, expected label = 1
------------------------------------------------------------------------
Classifying line:
  604	The company shipped my product very promptly and it works well.	1
  words = [compani, ship, product, promptli, work, well, compani ship, ship product, product promptli, promptli work, work well]
  predicted label = 0.539744572807, expected label = 1
------------------------------------------------------------------------
Classifying line:
  605	Exactly what I wanted.	1
  words = [exactli, want, exactli want]
  predicted label = 0.806228350854, expected label = 1
------------------------------------------------------------------------
Classifying line:
  606	This is a great deal.	1
  words = [great, deal, great deal]
  predicted label = 0.970762517884, expected label = 1
------------------------------------------------------------------------
Classifying line:
  607	Excellent product, I am very satisfied with the purchase.	1
  words = [excel, product, satisfi, purchas, excel product, product satisfi, satisfi purchas]
  predicted label = 0.915523883372, expected label = 1
------------------------------------------------------------------------
Classifying line:
  608	I highly recommend these and encourage people to give them a try.	1
  words = [highli, recommend, encourag, peopl, give, tri, highli recommend, recommend encourag, encourag peopl, peopl give, give tri]
  predicted label = 0.981735456049, expected label = 1
------------------------------------------------------------------------
Classifying line:
  609	Better than you'd expect.	1
  words = [better, expect, better expect]
  predicted label = 0.199185127815, expected label = 1
------------------------------------------------------------------------
Classifying line:
  610	Great product and price.	1
  words = [great, product, price, great product, product price]
  predicted label = 0.988661236483, expected label = 1

================================
You guessed 324/600 = 54.0% correct.
  - False positive rate: [0.0, 0.0025, 0.0025, 0.005, 0.005, 0.0075, 0.0075, 0.01, 0.01, 0.0125, 0.0125, 0.0125, 0.0125, 0.015, 0.015, 0.0225, 0.0225, 0.025, 0.025, 0.03, 0.03, 0.0375, 0.0375, 0.04, 0.04, 0.0425, 0.0425, 0.0525, 0.0525, 0.0525, 0.0575, 0.0575, 0.065, 0.065, 0.0675, 0.0675, 0.07, 0.07, 0.0725, 0.0725, 0.0775, 0.0775, 0.0875, 0.0875, 0.0925, 0.0925, 0.0975, 0.0975, 0.1, 0.1, 0.11, 0.11, 0.11, 0.1125, 0.1125, 0.115, 0.115, 0.1175, 0.1175, 0.1225, 0.1225, 0.125, 0.125, 0.1275, 0.1275, 0.1325, 0.1325, 0.1375, 0.1375, 0.1425, 0.1425, 0.155, 0.155, 0.1575, 0.1575, 0.1575, 0.16, 0.16, 0.1625, 0.1625, 0.1675, 0.1675, 0.1775, 0.18, 0.18, 0.1825, 0.1825, 0.1825, 0.185, 0.185, 0.2075, 0.2075, 0.21, 0.21, 0.2125, 0.2125, 0.2175, 0.2175, 0.225, 0.2425, 0.2475, 0.2525, 0.2525, 0.2575, 0.26, 0.2675, 0.2675, 0.2775, 0.2775, 0.295, 0.295, 0.31, 0.31, 0.3125, 0.3125, 0.32, 0.32, 0.325, 0.335, 0.335, 0.3375, 0.3625, 0.3625, 0.365, 0.37, 0.3725, 0.3725, 0.375, 0.375, 0.3825, 0.3825, 0.385, 0.385, 0.39, 0.395, 0.4125, 0.4125, 0.4275, 0.4325, 0.4375, 0.4375, 0.44, 0.44, 0.445, 0.445, 0.4475, 0.4475, 0.45, 0.45, 0.4575, 0.4575, 0.4625, 0.535, 0.54, 0.54, 0.5425, 0.5425, 0.55, 0.55, 0.555, 0.555, 0.5725, 0.5725, 0.5925, 0.5925, 0.595, 0.6, 0.61, 0.6125, 0.6125, 0.62, 0.63, 0.63, 0.635, 0.635, 0.64, 0.645, 0.645, 0.65, 0.65, 0.6525, 0.6525, 0.66, 0.6625, 0.6625, 0.69, 0.69, 0.6975, 0.6975, 0.6975, 0.6975, 0.7075, 0.715, 0.72, 0.7275, 0.7275, 0.7325, 0.735, 0.74, 0.74, 0.7675, 0.7675, 0.79, 0.7925, 0.7975, 0.7975, 0.82, 0.82, 0.8225, 0.8225, 0.825, 0.825, 0.8325, 0.8325, 0.8375, 0.8375, 0.8425, 0.8425, 0.85, 0.85, 0.8525, 0.8525, 0.855, 0.86, 0.8625, 0.8625, 0.8675, 0.875, 0.8775, 0.8775, 0.8825, 0.8825, 0.8875, 0.8875, 0.8975, 0.8975, 0.905, 0.905, 0.91, 0.9125, 0.9125, 0.9325, 0.9325, 0.9625, 0.9625, 0.965, 0.965, 1.0]
  - True positive rate: [0.0, 0.0, 0.005, 0.005, 0.025, 0.025, 0.03, 0.03, 0.035, 0.035, 0.04, 0.06, 0.075, 0.075, 0.09, 0.09, 0.11, 0.11, 0.12, 0.12, 0.125, 0.125, 0.135, 0.135, 0.15, 0.15, 0.16, 0.16, 0.175, 0.185, 0.185, 0.19, 0.19, 0.195, 0.195, 0.2, 0.2, 0.215, 0.215, 0.22, 0.22, 0.235, 0.235, 0.25, 0.25, 0.255, 0.255, 0.26, 0.26, 0.27, 0.27, 0.28, 0.285, 0.285, 0.29, 0.29, 0.31, 0.31, 0.315, 0.315, 0.325, 0.325, 0.34, 0.34, 0.345, 0.345, 0.35, 0.35, 0.365, 0.365, 0.38, 0.38, 0.39, 0.39, 0.395, 0.41, 0.41, 0.42, 0.42, 0.43, 0.43, 0.44, 0.44, 0.44, 0.455, 0.455, 0.465, 0.47, 0.47, 0.475, 0.475, 0.48, 0.48, 0.49, 0.495, 0.5, 0.5, 0.51, 0.51, 0.51, 0.51, 0.51, 0.515, 0.515, 0.525, 0.525, 0.53, 0.53, 0.54, 0.54, 0.55, 0.55, 0.555, 0.555, 0.56, 0.56, 0.565, 0.57, 0.57, 0.575, 0.58, 0.58, 0.59, 0.59, 0.59, 0.59, 0.595, 0.595, 0.6, 0.6, 0.61, 0.61, 0.615, 0.62, 0.62, 0.62, 0.625, 0.625, 0.625, 0.625, 0.63, 0.63, 0.635, 0.635, 0.64, 0.64, 0.645, 0.645, 0.66, 0.66, 0.665, 0.665, 0.7, 0.7, 0.715, 0.715, 0.72, 0.72, 0.725, 0.725, 0.73, 0.73, 0.745, 0.745, 0.765, 0.765, 0.765, 0.765, 0.775, 0.78, 0.78, 0.78, 0.785, 0.785, 0.79, 0.79, 0.79, 0.795, 0.795, 0.805, 0.805, 0.815, 0.815, 0.815, 0.82, 0.82, 0.825, 0.825, 0.83, 0.84, 0.845, 0.845, 0.845, 0.845, 0.845, 0.85, 0.85, 0.855, 0.855, 0.86, 0.86, 0.87, 0.87, 0.875, 0.875, 0.885, 0.885, 0.89, 0.89, 0.895, 0.895, 0.9, 0.9, 0.905, 0.905, 0.91, 0.91, 0.92, 0.92, 0.93, 0.93, 0.935, 0.935, 0.935, 0.935, 0.94, 0.94, 0.94, 0.94, 0.945, 0.945, 0.955, 0.955, 0.96, 0.96, 0.965, 0.965, 0.97, 0.97, 0.97, 0.98, 0.98, 0.99, 0.99, 0.995, 0.995, 1.0, 1.0]
  - Thresholds: [1.9995721566360478, 0.9995721566360479, 0.9971761912514021, 0.9969044554761072, 0.995573594620517, 0.9948649535214048, 0.992060158082144, 0.9911142566829845, 0.9905267203741354, 0.9898967044281213, 0.9894824574028463, 0.9886612364830479, 0.987543886521868, 0.9874769622819536, 0.9848877499025561, 0.9835488331542658, 0.9789926583445124, 0.978624211859191, 0.9780342727687827, 0.9763066894284914, 0.9761184043474, 0.9758417007656733, 0.9754667975184953, 0.9747956946314926, 0.9707625178844037, 0.9707331496649233, 0.9668420919171732, 0.962045957309432, 0.960800238985646, 0.9596223484449405, 0.9594203696881486, 0.9575412718605955, 0.9564730141243749, 0.9546855933556638, 0.9537481678325452, 0.9534632623087357, 0.9518876740156946, 0.9512362264783892, 0.9483573348629183, 0.9474814330048787, 0.9465213867952068, 0.9449752815676536, 0.9413836259299506, 0.9376797153994382, 0.9372275009847525, 0.935073604457416, 0.934073719318592, 0.9323775854649766, 0.9322125587088811, 0.9277463511368294, 0.9206490951783098, 0.918838203440924, 0.9184292858416224, 0.9163994748036478, 0.915523883372141, 0.9144613867449808, 0.9114042212038822, 0.9108533741292187, 0.9086201784036938, 0.9080268297015213, 0.906931275040426, 0.9068278257422434, 0.8926006339188125, 0.891712261562591, 0.8899994129422305, 0.8872332817117692, 0.8857983042538142, 0.8830266548036677, 0.8812998916726965, 0.8778414007585337, 0.8772014314239807, 0.8730744419147759, 0.8671946610662654, 0.863745903678479, 0.8620574064885843, 0.8609242686627298, 0.8582132557167659, 0.8562056289935515, 0.8548120927468513, 0.8508797349700107, 0.8483034871611898, 0.845083904180928, 0.8440593350792415, 0.8439892483370339, 0.8391471076317922, 0.8382310132074703, 0.8344950367317131, 0.8291993692624718, 0.8248577163560352, 0.8245955959270993, 0.8160160547162504, 0.8144361200427593, 0.8132642015706908, 0.8057787298249105, 0.804949779056689, 0.8049323157324585, 0.7895530062869893, 0.7837769861073172, 0.7837254738351861, 0.7667565139981775, 0.7581078666414856, 0.7561789529986684, 0.754140930945626, 0.7536830523961736, 0.7534568276840186, 0.7469516011431672, 0.744550172946045, 0.7392568321673186, 0.735935943862931, 0.7142491190639927, 0.7064406782899059, 0.6954391223095511, 0.6953913617550063, 0.6943632497333423, 0.6927075309875993, 0.685930458236242, 0.6855690335423574, 0.6854902405285013, 0.669136303112237, 0.6676003310425311, 0.6667380580117128, 0.6401131472315059, 0.6372043668112921, 0.6325763225702922, 0.6298720296893197, 0.6290456626585532, 0.6271645760417398, 0.626433357979602, 0.6198768274768712, 0.6077676294952162, 0.6065987034848268, 0.6058668040127815, 0.6048522838688698, 0.6044251965745996, 0.6040018171681989, 0.583799402249207, 0.5803157307275738, 0.5687056329421188, 0.5664164322249465, 0.5634009590300315, 0.5627343671872714, 0.5606716840468572, 0.5601665668749237, 0.5526287918425508, 0.5493668578033917, 0.5469909681975164, 0.5403209855092033, 0.5402188947933215, 0.5380480825494727, 0.5213691706015678, 0.5179855123266461, 0.5071147188283317, 0.506892600390953, 0.503561057870539, 0.49025571939125623, 0.4876338350710842, 0.4852612147883022, 0.4770541385631452, 0.4752607535756945, 0.4744602122080074, 0.47439865630862854, 0.458846775726299, 0.4481796042327732, 0.43076929307781153, 0.4213018459904595, 0.420929320884059, 0.41991164076747656, 0.4030546413472465, 0.40291670667838747, 0.3988796374397073, 0.3945528162494944, 0.3816335445535317, 0.3765345698280046, 0.369428425759657, 0.36640175851931966, 0.3645289380931909, 0.36262548820046736, 0.3625183537442533, 0.3577439803898989, 0.34833601742074843, 0.3481643086819973, 0.34473290029691916, 0.3424783984737136, 0.33887962188265935, 0.33792867197749565, 0.3167411125691413, 0.31267629880946846, 0.3025532776075224, 0.29963049015016546, 0.29931018339766385, 0.29894578771964647, 0.29365699609237506, 0.2923790730730698, 0.2921041517877735, 0.2902300054412096, 0.28715064625371517, 0.2845570151236924, 0.2821454182310243, 0.27940285091233896, 0.2746061617778465, 0.2452306364335481, 0.2414423333232433, 0.2144921371182103, 0.2058729031733709, 0.20321786242334777, 0.19918512781549008, 0.1661536711873698, 0.16583811613153304, 0.164349160998523, 0.16147418635930236, 0.1548931443100324, 0.15257524993232718, 0.1449034176025682, 0.14133535209499262, 0.1337942424951655, 0.13375840488465984, 0.1275955362712292, 0.12634085682551305, 0.11793382071754971, 0.11521073966072995, 0.11185999384007203, 0.11164818112945174, 0.11035506567572663, 0.10945100478142726, 0.10713751474499454, 0.10575996547797488, 0.10497312569661993, 0.10491243747098022, 0.10403567332134622, 0.10397183630865668, 0.10294588177305403, 0.10112127059453145, 0.0948292764654682, 0.09321829575381252, 0.08722041711796402, 0.0864112928181541, 0.08433159802222831, 0.08105873705920849, 0.07942865793100608, 0.07747811041209536, 0.07082154901193062, 0.05040282486488819, 0.04681510067451015, 0.02442511501479201, 0.023177149921865845, 0.021399380667337688, 0.02041644758446738, 0.0002711128570598564]
  - AUC: 0.65779375
  - F1: 0.503597122302
================================

Running model 'mnb' on train data 'default.txt2.train' and test data 'default.txt2.test'
------------------------------------------------------------------------
Classifying line:
  601	Magical Help.	1
  words = [magic, help, magic help]
  predicted label = 0.571906360526, expected label = 1
------------------------------------------------------------------------
Classifying line:
  602	The best phone in market :).	1
  words = [best, phone, market, best phone, phone market]
  predicted label = 0.880905302215, expected label = 1
------------------------------------------------------------------------
Classifying line:
  603	It worked very well.	1
  words = [work, well, work well]
  predicted label = 0.873674243294, expected label = 1
------------------------------------------------------------------------
Classifying line:
  604	The company shipped my product very promptly and it works well.	1
  words = [compani, ship, product, promptli, work, well, compani ship, ship product, product promptli, promptli work, work well]
  predicted label = 0.545945469811, expected label = 1
------------------------------------------------------------------------
Classifying line:
  605	Exactly what I wanted.	1
  words = [exactli, want, exactli want]
  predicted label = 0.804277183712, expected label = 1
------------------------------------------------------------------------
Classifying line:
  606	This is a great deal.	1
  words = [great, deal, great deal]
  predicted label = 0.970701087793, expected label = 1
------------------------------------------------------------------------
Classifying line:
  607	Excellent product, I am very satisfied with the purchase.	1
  words = [excel, product, satisfi, purchas, excel product, product satisfi, satisfi purchas]
  predicted label = 0.921330515649, expected label = 1
------------------------------------------------------------------------
Classifying line:
  608	I highly recommend these and encourage people to give them a try.	1
  words = [highli, recommend, encourag, peopl, give, tri, highli recommend, recommend encourag, encourag peopl, peopl give, give tri]
  predicted label = 0.979778916657, expected label = 1
------------------------------------------------------------------------
Classifying line:
  609	Better than you'd expect.	1
  words = [better, expect, better expect]
  predicted label = 0.22859822349, expected label = 1
------------------------------------------------------------------------
Classifying line:
  610	Great product and price.	1
  words = [great, product, price, great product, product price]
  predicted label = 0.988030577266, expected label = 1

================================
You guessed 333/600 = 55.5% correct.
  - False positive rate: [0.0, 0.0025, 0.0025, 0.0075, 0.0075, 0.01, 0.01, 0.0125, 0.0125, 0.0175, 0.0175, 0.0175, 0.02, 0.02, 0.025, 0.025, 0.03, 0.03, 0.035, 0.035, 0.035, 0.035, 0.04, 0.04, 0.05, 0.05, 0.055, 0.055, 0.0575, 0.0575, 0.06, 0.06, 0.0725, 0.0725, 0.075, 0.075, 0.0775, 0.0775, 0.08, 0.08, 0.085, 0.085, 0.0875, 0.0875, 0.0925, 0.0925, 0.095, 0.095, 0.1025, 0.1025, 0.1025, 0.1075, 0.1075, 0.11, 0.11, 0.115, 0.115, 0.12, 0.12, 0.1225, 0.1225, 0.13, 0.13, 0.1325, 0.1325, 0.135, 0.135, 0.1375, 0.1375, 0.1475, 0.1475, 0.1525, 0.1525, 0.155, 0.155, 0.155, 0.165, 0.175, 0.175, 0.1775, 0.1775, 0.18, 0.18, 0.185, 0.185, 0.185, 0.185, 0.1975, 0.1975, 0.2, 0.2025, 0.22, 0.22, 0.2275, 0.2275, 0.23, 0.235, 0.2375, 0.2375, 0.2425, 0.2475, 0.25, 0.2575, 0.2575, 0.26, 0.26, 0.27, 0.27, 0.2875, 0.2875, 0.3025, 0.3025, 0.305, 0.305, 0.3125, 0.315, 0.315, 0.325, 0.3275, 0.3425, 0.3425, 0.3525, 0.3525, 0.355, 0.36, 0.365, 0.365, 0.37, 0.37, 0.37, 0.375, 0.385, 0.3975, 0.4025, 0.4025, 0.41, 0.41, 0.415, 0.4175, 0.4175, 0.4225, 0.4225, 0.425, 0.4975, 0.5025, 0.5075, 0.5075, 0.51, 0.51, 0.52, 0.52, 0.525, 0.525, 0.53, 0.53, 0.545, 0.545, 0.5475, 0.5475, 0.555, 0.555, 0.5625, 0.5625, 0.5725, 0.5775, 0.585, 0.585, 0.59, 0.5925, 0.5925, 0.6, 0.6025, 0.6025, 0.6225, 0.6225, 0.625, 0.625, 0.6275, 0.635, 0.64, 0.64, 0.6425, 0.6425, 0.65, 0.65, 0.6525, 0.6525, 0.6725, 0.6725, 0.675, 0.6825, 0.6875, 0.6925, 0.6975, 0.7, 0.7025, 0.7025, 0.7075, 0.715, 0.7175, 0.7175, 0.725, 0.725, 0.735, 0.735, 0.76, 0.76, 0.7625, 0.7625, 0.7625, 0.77, 0.77, 0.775, 0.775, 0.7875, 0.7875, 0.795, 0.795, 0.805, 0.805, 0.8075, 0.8075, 0.825, 0.825, 0.8275, 0.8275, 0.8325, 0.8325, 0.845, 0.845, 0.8475, 0.855, 0.8575, 0.8575, 0.8675, 0.8675, 0.8725, 0.8725, 0.8775, 0.8825, 0.8825, 0.885, 0.885, 0.9175, 0.9175, 0.9225, 0.9225, 0.925, 0.925, 0.9375, 0.9375, 0.945, 0.945, 0.955, 0.955, 1.0]
  - True positive rate: [0.005, 0.005, 0.015, 0.015, 0.02, 0.02, 0.03, 0.03, 0.035, 0.035, 0.055, 0.065, 0.065, 0.09, 0.09, 0.095, 0.095, 0.11, 0.11, 0.115, 0.125, 0.14, 0.14, 0.155, 0.155, 0.17, 0.17, 0.175, 0.175, 0.18, 0.18, 0.195, 0.195, 0.2, 0.2, 0.21, 0.21, 0.22, 0.22, 0.23, 0.23, 0.235, 0.235, 0.24, 0.24, 0.25, 0.25, 0.26, 0.26, 0.27, 0.28, 0.28, 0.285, 0.285, 0.295, 0.295, 0.305, 0.305, 0.325, 0.325, 0.335, 0.335, 0.345, 0.345, 0.355, 0.355, 0.365, 0.365, 0.37, 0.37, 0.375, 0.375, 0.39, 0.39, 0.405, 0.415, 0.415, 0.415, 0.425, 0.425, 0.435, 0.435, 0.44, 0.44, 0.445, 0.455, 0.465, 0.465, 0.475, 0.475, 0.48, 0.48, 0.485, 0.485, 0.49, 0.49, 0.49, 0.49, 0.5, 0.5, 0.5, 0.51, 0.51, 0.525, 0.525, 0.53, 0.53, 0.535, 0.535, 0.54, 0.54, 0.55, 0.55, 0.555, 0.56, 0.56, 0.57, 0.57, 0.575, 0.575, 0.585, 0.585, 0.59, 0.59, 0.59, 0.59, 0.595, 0.595, 0.605, 0.615, 0.615, 0.62, 0.62, 0.62, 0.625, 0.625, 0.63, 0.63, 0.63, 0.64, 0.64, 0.655, 0.655, 0.69, 0.69, 0.69, 0.705, 0.705, 0.71, 0.71, 0.715, 0.715, 0.72, 0.72, 0.725, 0.725, 0.73, 0.73, 0.745, 0.745, 0.75, 0.75, 0.755, 0.755, 0.755, 0.755, 0.76, 0.76, 0.765, 0.77, 0.77, 0.77, 0.775, 0.775, 0.78, 0.78, 0.785, 0.785, 0.785, 0.785, 0.79, 0.79, 0.8, 0.8, 0.805, 0.805, 0.815, 0.815, 0.82, 0.82, 0.82, 0.82, 0.82, 0.82, 0.825, 0.825, 0.84, 0.84, 0.84, 0.84, 0.845, 0.845, 0.85, 0.85, 0.855, 0.855, 0.86, 0.86, 0.865, 0.875, 0.875, 0.885, 0.885, 0.89, 0.89, 0.895, 0.895, 0.9, 0.9, 0.905, 0.905, 0.91, 0.91, 0.915, 0.915, 0.925, 0.925, 0.935, 0.935, 0.94, 0.94, 0.94, 0.94, 0.945, 0.945, 0.955, 0.955, 0.96, 0.96, 0.96, 0.965, 0.965, 0.97, 0.97, 0.975, 0.975, 0.98, 0.98, 0.985, 0.985, 0.99, 0.99, 0.995, 0.995, 1.0, 1.0]
  - Thresholds: [0.9997866942738922, 0.9992504684367443, 0.9966643008903489, 0.99638681817464, 0.9963626403509301, 0.9963211267189728, 0.9950430420600878, 0.9938755478230684, 0.9920128743058901, 0.9893473533334232, 0.9880305772655373, 0.9869516084816162, 0.9853256769391044, 0.9829812263133703, 0.9820012768526389, 0.9810211152922405, 0.9804970542409999, 0.9781241620801501, 0.9766982458074203, 0.9764227124589006, 0.9754644289036346, 0.9740265132300423, 0.9724818084876645, 0.9707010877930613, 0.9658163415926424, 0.9649420272531729, 0.9624972165800595, 0.9616975510424048, 0.9606742235337179, 0.9604991956163671, 0.9596018516932092, 0.9549314150210111, 0.9522669962756833, 0.9522404930490056, 0.952147492834803, 0.9484385388427619, 0.9481097674707076, 0.9476384644346333, 0.9460293504137243, 0.94442950111768, 0.94298803898847, 0.9423996233267284, 0.94185985559056, 0.9413512214616693, 0.9401746419743069, 0.9361803239791557, 0.9344808249197828, 0.93313031812495, 0.9324953746890549, 0.9297125160352404, 0.923588608248001, 0.9229106458667883, 0.9225832946904055, 0.9221333421095171, 0.9196844765355136, 0.9147818628412909, 0.9140762357287621, 0.9130965980177284, 0.9035034018536346, 0.9015783593958872, 0.8977876842569337, 0.8957048096499486, 0.8921180190089195, 0.8917741320528186, 0.8902051897625909, 0.8891378955408152, 0.8883105839616297, 0.887597900650506, 0.887102398035397, 0.8828169170138539, 0.8809053022151062, 0.8789075647458777, 0.8756037073803662, 0.8752813244636617, 0.8736742432942828, 0.8727616559961922, 0.8619379470398575, 0.8575307645150196, 0.8568240185971284, 0.8528625913648175, 0.8513109888360737, 0.8485498273031725, 0.8481197450978595, 0.8422506173549057, 0.8400480715769642, 0.8397259853261755, 0.8340800960822409, 0.8269059506768685, 0.8263108682016872, 0.821968953576083, 0.8218156543854783, 0.8087105992833181, 0.804277183711574, 0.8032282898141024, 0.799264868425662, 0.7975607962261895, 0.7924003747521625, 0.7917462531541317, 0.7853250044259604, 0.7762847327908625, 0.7736155335540252, 0.7695769620491936, 0.7639308676071074, 0.7551317984120851, 0.7550190332017057, 0.7515653227224546, 0.7411152893100899, 0.7405398402161659, 0.7304575170995699, 0.728189958686756, 0.713341658936351, 0.7092170585574677, 0.7084547130574655, 0.7048048648952072, 0.7046316187571103, 0.7033496026022743, 0.6957177158726849, 0.6889102109849774, 0.6856455546097849, 0.6578753483524179, 0.6561173765198982, 0.6522135864529686, 0.6513065265414935, 0.651124867489719, 0.6489801970668836, 0.6449780668660753, 0.6433848340676366, 0.6425768247882709, 0.6392707251644169, 0.636289837221997, 0.6305772706295907, 0.625457211310735, 0.5916450809401688, 0.5887078324304489, 0.5793245721854324, 0.5729824085010798, 0.5719063605256351, 0.5709336797210783, 0.5595020844560337, 0.5585851828180479, 0.5544142031882757, 0.5459454698113168, 0.5451518688243641, 0.5416666666666667, 0.54050051025748, 0.5361202650960561, 0.5343987598526299, 0.5291076277786395, 0.525954448016169, 0.5210311338750009, 0.5161669815488039, 0.5123349905020141, 0.5119523185266406, 0.5027206951451935, 0.5011573100877713, 0.4794161081794526, 0.4792997295793733, 0.4761751368794225, 0.4698000821961148, 0.46658729588537784, 0.4652530384588087, 0.45907934815061413, 0.4589372041637592, 0.4434073161207162, 0.4429592848888537, 0.44212037006740884, 0.432911398432088, 0.42907240072147157, 0.4260089746129831, 0.4239929047801713, 0.41714100450531766, 0.41670402690925695, 0.4156213221375592, 0.39352917873048116, 0.3931049203862937, 0.385711648366728, 0.38133740681254147, 0.3800931128195405, 0.37791590775645306, 0.3747139355252411, 0.3699697158046473, 0.3695046150259696, 0.3625676476198806, 0.35037144840548773, 0.3472206444522819, 0.3464624417367554, 0.3457650021834843, 0.32584065011056623, 0.3244477975291827, 0.3244180988648046, 0.3230071833057419, 0.3076567459049037, 0.30643117338303627, 0.30558770007862396, 0.30252636566421265, 0.302357633578458, 0.29702879409879596, 0.2933438536700024, 0.2917830758878531, 0.2907965139785683, 0.2896852724133223, 0.28460456153977526, 0.2806768209436359, 0.26999356343361264, 0.2688002727681067, 0.24301860696244706, 0.2421408525037935, 0.23055260468644798, 0.23039343900294698, 0.22859822349046954, 0.2241643572491331, 0.22255965717624066, 0.21423202618229129, 0.20805790668835547, 0.19261095222963687, 0.18820625753344597, 0.18042142775824427, 0.17930976006645374, 0.16778490638580187, 0.16602580798684624, 0.1597286290536362, 0.15490772079717566, 0.1321396792214853, 0.131051788871366, 0.13050158418508936, 0.12364846313099799, 0.11648208499521154, 0.11504189506327522, 0.10880721080366135, 0.10867346172104471, 0.10718379760142517, 0.10656861756866688, 0.10557751131722709, 0.10539086166606773, 0.09587593828807069, 0.09348509299524886, 0.08785001364509848, 0.08739801179634753, 0.08711156588505592, 0.08559189862608237, 0.08469552966227063, 0.08432622739956486, 0.07836606787725096, 0.04902794117100507, 0.04776310027410024, 0.044764158603463514, 0.04319931288834804, 0.03839239509216681, 0.03709227157140396, 0.028131093151712264, 0.027909421503115157, 0.02389384907770943, 0.02244073098572505, 0.01981814638207718, 0.019344299077687924, 0.00012922626925942644]
  - AUC: 0.661625
  - F1: 0.520646319569
================================

Running model 'dt' on train data 'default.txt2.train' and test data 'default.txt2.test'
Best hyperparameters were: {'max_features': 'auto', 'min_samples_leaf': 1}
------------------------------------------------------------------------
Classifying line:
  601	Magical Help.	1
  words = [magic, help, magic help]
  predicted label = 0.440909090909, expected label = 1
------------------------------------------------------------------------
Classifying line:
  602	The best phone in market :).	1
  words = [best, phone, market, best phone, phone market]
  predicted label = 1.0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  603	It worked very well.	1
  words = [work, well, work well]
  predicted label = 1.0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  604	The company shipped my product very promptly and it works well.	1
  words = [compani, ship, product, promptli, work, well, compani ship, ship product, product promptli, promptli work, work well]
  predicted label = 1.0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  605	Exactly what I wanted.	1
  words = [exactli, want, exactli want]
  predicted label = 1.0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  606	This is a great deal.	1
  words = [great, deal, great deal]
  predicted label = 1.0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  607	Excellent product, I am very satisfied with the purchase.	1
  words = [excel, product, satisfi, purchas, excel product, product satisfi, satisfi purchas]
  predicted label = 1.0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  608	I highly recommend these and encourage people to give them a try.	1
  words = [highli, recommend, encourag, peopl, give, tri, highli recommend, recommend encourag, encourag peopl, peopl give, give tri]
  predicted label = 1.0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  609	Better than you'd expect.	1
  words = [better, expect, better expect]
  predicted label = 0.0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  610	Great product and price.	1
  words = [great, product, price, great product, product price]
  predicted label = 1.0, expected label = 1

================================
You guessed 326/600 = 54.3333333333% correct.
  - False positive rate: [0.0, 0.4275, 0.4275, 0.43, 0.4425, 0.515, 0.515, 0.535, 0.6325, 0.64, 0.695, 1.0]
  - True positive rate: [0.0, 0.545, 0.56, 0.58, 0.625, 0.65, 0.655, 0.7, 0.74, 0.745, 0.745, 1.0]
  - Thresholds: [2.0, 1.0, 0.875, 0.75, 0.7, 0.6666666666666666, 0.5714285714285714, 0.5, 0.4409090909090909, 0.3333333333333333, 0.16666666666666666, 0.0]
  - AUC: 0.568075
  - F1: 0.505415162455
================================

Running model 'rf' on train data 'default.txt2.train' and test data 'default.txt2.test'
Best hyperparameters were: {'n_estimators': 30}
------------------------------------------------------------------------
Classifying line:
  601	Magical Help.	1
  words = [magic, help, magic help]
  predicted label = 0.481033357317, expected label = 1
------------------------------------------------------------------------
Classifying line:
  602	The best phone in market :).	1
  words = [best, phone, market, best phone, phone market]
  predicted label = 0.957428511134, expected label = 1
------------------------------------------------------------------------
Classifying line:
  603	It worked very well.	1
  words = [work, well, work well]
  predicted label = 0.736868950747, expected label = 1
------------------------------------------------------------------------
Classifying line:
  604	The company shipped my product very promptly and it works well.	1
  words = [compani, ship, product, promptli, work, well, compani ship, ship product, product promptli, promptli work, work well]
  predicted label = 0.736868950747, expected label = 1
------------------------------------------------------------------------
Classifying line:
  605	Exactly what I wanted.	1
  words = [exactli, want, exactli want]
  predicted label = 0.481155380998, expected label = 1
------------------------------------------------------------------------
Classifying line:
  606	This is a great deal.	1
  words = [great, deal, great deal]
  predicted label = 0.993562574933, expected label = 1
------------------------------------------------------------------------
Classifying line:
  607	Excellent product, I am very satisfied with the purchase.	1
  words = [excel, product, satisfi, purchas, excel product, product satisfi, satisfi purchas]
  predicted label = 0.952983941657, expected label = 1
------------------------------------------------------------------------
Classifying line:
  608	I highly recommend these and encourage people to give them a try.	1
  words = [highli, recommend, encourag, peopl, give, tri, highli recommend, recommend encourag, encourag peopl, peopl give, give tri]
  predicted label = 0.577809787746, expected label = 1
------------------------------------------------------------------------
Classifying line:
  609	Better than you'd expect.	1
  words = [better, expect, better expect]
  predicted label = 0.40969150839, expected label = 1
------------------------------------------------------------------------
Classifying line:
  610	Great product and price.	1
  words = [great, product, price, great product, product price]
  predicted label = 0.993562574933, expected label = 1

================================
You guessed 326/600 = 54.3333333333% correct.
  - False positive rate: [0.0, 0.0125, 0.0125, 0.0125, 0.025, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.0375, 0.0525, 0.0575, 0.1375, 0.1425, 0.1525, 0.155, 0.155, 0.17, 0.1825, 0.185, 0.2, 0.225, 0.24, 0.2475, 0.25, 0.295, 0.3025, 0.3075, 0.31, 0.31, 0.3175, 0.325, 0.33, 0.335, 0.335, 0.35, 0.35, 0.3575, 0.36, 0.36, 0.3625, 0.3625, 0.3675, 0.3675, 0.3725, 0.3725, 0.375, 0.375, 0.38, 0.3875, 0.3875, 0.395, 0.395, 0.4, 0.4, 0.4075, 0.4125, 0.4125, 0.4275, 0.4325, 0.4375, 0.4425, 0.4425, 0.445, 0.445, 0.4475, 0.45, 0.46, 0.46, 0.465, 0.47, 0.47, 0.4725, 0.4725, 0.4775, 0.485, 0.49, 0.495, 0.495, 0.5, 0.5025, 0.505, 0.505, 0.505, 0.74, 0.74, 0.745, 0.76, 0.76, 0.765, 0.765, 0.7675, 0.7675, 0.775, 0.78, 0.7825, 0.7825, 0.785, 0.785, 0.79, 0.79, 0.795, 0.7975, 0.7975, 0.7975, 0.8, 0.8, 0.8025, 0.8025, 0.8075, 0.8125, 0.82, 0.825, 0.8275, 0.835, 0.835, 0.84, 0.8425, 0.8475, 0.85, 0.85, 0.8525, 0.8525, 0.8575, 0.8575, 0.87, 0.8725, 0.875, 0.875, 0.88, 0.88, 0.885, 0.885, 0.89, 0.89, 0.89, 0.89, 0.895, 0.895, 0.8975, 0.8975, 0.9025, 0.9025, 0.905, 0.94, 0.955, 0.955, 0.98, 0.995, 1.0]
  - True positive rate: [0.0, 0.16, 0.165, 0.175, 0.215, 0.24, 0.26, 0.265, 0.275, 0.285, 0.295, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.305, 0.305, 0.305, 0.305, 0.305, 0.39, 0.39, 0.39, 0.39, 0.39, 0.39, 0.39, 0.39, 0.41, 0.41, 0.41, 0.41, 0.41, 0.415, 0.415, 0.425, 0.425, 0.43, 0.435, 0.435, 0.44, 0.44, 0.445, 0.445, 0.45, 0.45, 0.455, 0.455, 0.455, 0.465, 0.465, 0.475, 0.475, 0.48, 0.48, 0.48, 0.505, 0.505, 0.505, 0.52, 0.525, 0.53, 0.53, 0.535, 0.535, 0.54, 0.54, 0.545, 0.545, 0.555, 0.565, 0.565, 0.575, 0.575, 0.58, 0.58, 0.58, 0.585, 0.585, 0.585, 0.595, 0.6, 0.61, 0.73, 0.74, 0.74, 0.74, 0.745, 0.745, 0.75, 0.75, 0.76, 0.76, 0.765, 0.765, 0.77, 0.77, 0.775, 0.78, 0.79, 0.79, 0.79, 0.8, 0.81, 0.81, 0.82, 0.82, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.835, 0.835, 0.84, 0.84, 0.84, 0.855, 0.855, 0.86, 0.86, 0.865, 0.865, 0.885, 0.885, 0.89, 0.89, 0.905, 0.905, 0.915, 0.915, 0.92, 0.97, 0.975, 0.975, 0.985, 0.985, 0.99, 0.99, 0.995, 0.995, 0.995, 0.995, 1.0, 1.0, 1.0, 1.0]
  - Thresholds: [1.9935625749333354, 0.9935625749333356, 0.9758029024131344, 0.9677352481148488, 0.9574285111344256, 0.9554271609612586, 0.9529839416572153, 0.9473185814481819, 0.9435027919744978, 0.9364852481148488, 0.9270031133669213, 0.923669780033588, 0.8504874148269654, 0.8418852642893311, 0.8304407839020792, 0.8283705932015571, 0.812734887929711, 0.8042879655027576, 0.8041281689591331, 0.8040265684797863, 0.7877427456978842, 0.7790403692950623, 0.7784159117115645, 0.7752231922341022, 0.7733658335988204, 0.755601558832204, 0.7543175264171806, 0.753991271095037, 0.7503120315875607, 0.7401949022081362, 0.7401268464023755, 0.7397300210055501, 0.736868950747059, 0.730588480285062, 0.7305367779928812, 0.7227768761112286, 0.7187165899921191, 0.7126698052806199, 0.6979108991072553, 0.6900116615845898, 0.6833925492760393, 0.683015337328554, 0.6803008399310829, 0.6798466701060157, 0.6781994416384192, 0.6740145532652319, 0.6727152664089109, 0.6661533309246216, 0.660502339348562, 0.6593211297021765, 0.6505827393663568, 0.6393692099954265, 0.6223381173922866, 0.6139083530498913, 0.6064961319385269, 0.6046384452509792, 0.6008371801062954, 0.5999418176742672, 0.5803168078240787, 0.5795406613392157, 0.5733276865261552, 0.5723673756346576, 0.5613265736707582, 0.5591561897730367, 0.5539789034198621, 0.5521725234788819, 0.5501170756559338, 0.5489134327753065, 0.5442286901338117, 0.5295656560139436, 0.5238109175780357, 0.5223367000803735, 0.5148261865513036, 0.5139284061346511, 0.5086849636554115, 0.5048808106957977, 0.5022226385262257, 0.49776352783529026, 0.4971357251491571, 0.49535580879626806, 0.49384749177337633, 0.4937713745345556, 0.4927363806622652, 0.49175250488876304, 0.48462999020085923, 0.4820997061574173, 0.48115538099847455, 0.48103335731745084, 0.47390820578585086, 0.47375287648810127, 0.45963258376756383, 0.4581548197572431, 0.4479830841310129, 0.44689948674566715, 0.44541316896532324, 0.442961004069379, 0.4400617459737148, 0.4310922819399399, 0.43009502141061456, 0.4298986221911872, 0.42538562885971387, 0.42316814411760983, 0.4225262919257304, 0.4180131779455696, 0.4175292134346077, 0.4128847468721873, 0.4096915083900528, 0.40925423353026774, 0.4085914626982627, 0.40665645284999485, 0.4039448128583959, 0.4000151449975943, 0.39226812723388954, 0.3910581593104373, 0.38856057290123097, 0.3880235259435986, 0.3856297042619143, 0.38390981222009113, 0.38243870312052525, 0.3787453681561666, 0.3696001592480153, 0.3669428355750456, 0.3652419825640524, 0.34665895974883104, 0.3416137268612264, 0.34028813492512106, 0.33871306485727454, 0.3383428369935535, 0.33048668744387727, 0.32849225319536807, 0.32085977601548216, 0.3172877714026509, 0.31688020137655426, 0.3117541097335141, 0.3004140716736995, 0.2973951559123621, 0.27476762463113685, 0.2644330609155968, 0.2563911866473074, 0.25394826214555943, 0.25194762049863134, 0.24337985157714884, 0.22659568642516603, 0.222299710470692, 0.19189631334872095, 0.17117875798158347, 0.13029512720404895, 0.12481950769965247, 0.10321017452558306, 0.09598935728709937, 0.06381697889693702, 0.004183006535947712, 0.0]
  - AUC: 0.58235
  - F1: 0.456349206349
================================

