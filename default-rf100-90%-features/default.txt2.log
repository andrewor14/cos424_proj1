preprocessSentences.py:68: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal
  return [clean_word(w) for w in tokens if w.lower() not in stop_words and w.isalpha()]
Path: .
Training data: ./default.txt2.train
Done building things up man. Num documents in train set: 2400.
Number of features before any feature selection: 10596
Number of features after filtering out words by count threshold: 514
Done doing the feature selection thing man. Num vocabs: 462.
Output files: ./out*
Runtime: 4.33670687675
Running model 'mybnb' on train data 'default.txt2.train' and test data 'default.txt2.test'
Num train examples = 2400, positive prior = 0.541666666667, negative prior = 0.458333333333
Using 462 features: unit, music, hold, want, hot, wrong, beauti, fit, funni, silent...
442 features are used in positive reviews: forget, lack, month, abil, go, tv, friendli, send, certainli, worth...
425 features are used in negative reviews: forget, lack, month, abil, go, hate, tv, friendli, send, certainli...
------------------------------------------------------------------------
Classifying line:
  601	Magical Help.	1
  words = [magic, help, magic help]
  positive probability = 1.71789730317e-09, negative probability = 1.71240569786e-09
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  602	The best phone in market :).	1
  words = [best, phone, market, best phone, phone market]
  positive probability = 2.31631261326e-13, negative probability = 5.33009899333e-14
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  603	It worked very well.	1
  words = [work, well, work well]
  positive probability = 3.36707871422e-07, negative probability = 5.75368314479e-08
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  604	The company shipped my product very promptly and it works well.	1
  words = [compani, ship, product, promptli, work, well, compani ship, ship product, product promptli, promptli work, work well]
  positive probability = 9.78535615368e-31, negative probability = 2.3808700668e-30
  predicted label = 0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  605	Exactly what I wanted.	1
  words = [exactli, want, exactli want]
  positive probability = 2.35597344435e-08, negative probability = 5.82217937271e-09
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  606	This is a great deal.	1
  words = [great, deal, great deal]
  positive probability = 1.20743639023e-07, negative probability = 4.10977367485e-09
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  607	Excellent product, I am very satisfied with the purchase.	1
  words = [excel, product, satisfi, purchas, excel product, product satisfi, satisfi purchas]
  positive probability = 3.68925749102e-20, negative probability = 6.96677101902e-21
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  608	I highly recommend these and encourage people to give them a try.	1
  words = [highli, recommend, encourag, peopl, give, tri, highli recommend, recommend encourag, encourag peopl, peopl give, give tri]
  positive probability = 4.7010897899e-28, negative probability = 1.79585627896e-29
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  609	Better than you'd expect.	1
  words = [better, expect, better expect]
  positive probability = 1.96331120363e-08, negative probability = 8.01405866596e-08
  predicted label = 0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  610	Great product and price.	1
  words = [great, product, price, great product, product price]
  positive probability = 3.84623709432e-12, negative probability = 5.92233221481e-14
  predicted label = 1, expected label = 1

================================
You guessed 396/600 = 66.0% correct.
  - False positive rate: [0.0, 0.2875, 1.0]
  - True positive rate: [0.0, 0.555, 1.0]
  - Thresholds: [2, 1, 0]
  - AUC: 0.63375
  - Precision: 0.491150442478
  - Recall: 0.555
  - F1: 0.521126760563
================================

Running model 'mymnb' on train data 'default.txt2.train' and test data 'default.txt2.test'
Num train examples = 2400, positive prior = 0.541666666667, negative prior = 0.458333333333
Using 462 features: unit, music, hold, want, hot, wrong, beauti, fit, funni, silent...
442 features are used in positive reviews: forget, lack, month, abil, go, tv, friendli, send, certainli, worth...
425 features are used in negative reviews: forget, lack, month, abil, go, hate, tv, friendli, send, certainli...
------------------------------------------------------------------------
Classifying line:
  601	Magical Help.	1
  words = [magic, help, magic help]
  positive probability = 6.93126396305e-10, negative probability = 6.01323752762e-10
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  602	The best phone in market :).	1
  words = [best, phone, market, best phone, phone market]
  positive probability = 5.97046881972e-14, negative probability = 9.61193681142e-15
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  603	It worked very well.	1
  words = [work, well, work well]
  positive probability = 1.49319229375e-07, negative probability = 2.02044780928e-08
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  604	The company shipped my product very promptly and it works well.	1
  words = [compani, ship, product, promptli, work, well, compani ship, ship product, product promptli, promptli work, work well]
  positive probability = 3.85726741954e-32, negative probability = 5.13147136371e-32
  predicted label = 0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  605	Exactly what I wanted.	1
  words = [exactli, want, exactli want]
  positive probability = 9.50573343504e-09, negative probability = 2.16476550994e-09
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  606	This is a great deal.	1
  words = [great, deal, great deal]
  positive probability = 5.10933172133e-08, negative probability = 1.44317700663e-09
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  607	Excellent product, I am very satisfied with the purchase.	1
  words = [excel, product, satisfi, purchas, excel product, product satisfi, satisfi purchas]
  positive probability = 4.68441782345e-21, negative probability = 6.06087729154e-22
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  608	I highly recommend these and encourage people to give them a try.	1
  words = [highli, recommend, encourag, peopl, give, tri, highli recommend, recommend encourag, encourag peopl, peopl give, give tri]
  positive probability = 1.79838698762e-29, negative probability = 3.87059554291e-31
  predicted label = 1, expected label = 1
------------------------------------------------------------------------
Classifying line:
  609	Better than you'd expect.	1
  words = [better, expect, better expect]
  positive probability = 8.91162509535e-09, negative probability = 2.81419516293e-08
  predicted label = 0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  610	Great product and price.	1
  words = [great, product, price, great product, product price]
  positive probability = 8.88681320473e-13, negative probability = 1.03513165661e-14
  predicted label = 1, expected label = 1

================================
You guessed 386/600 = 64.3333333333% correct.
  - False positive rate: [0.0, 0.355, 1.0]
  - True positive rate: [0.0, 0.64, 1.0]
  - Thresholds: [2, 1, 0]
  - AUC: 0.6425
  - Precision: 0.474074074074
  - Recall: 0.64
  - F1: 0.544680851064
================================

Running model 'bnb' on train data 'default.txt2.train' and test data 'default.txt2.test'
------------------------------------------------------------------------
Classifying line:
  601	Magical Help.	1
  words = [magic, help, magic help]
  predicted label = 0.549366857803, expected label = 1
------------------------------------------------------------------------
Classifying line:
  602	The best phone in market :).	1
  words = [best, phone, market, best phone, phone market]
  predicted label = 0.862057406489, expected label = 1
------------------------------------------------------------------------
Classifying line:
  603	It worked very well.	1
  words = [work, well, work well]
  predicted label = 0.860924268663, expected label = 1
------------------------------------------------------------------------
Classifying line:
  604	The company shipped my product very promptly and it works well.	1
  words = [compani, ship, product, promptli, work, well, compani ship, ship product, product promptli, promptli work, work well]
  predicted label = 0.539744572807, expected label = 1
------------------------------------------------------------------------
Classifying line:
  605	Exactly what I wanted.	1
  words = [exactli, want, exactli want]
  predicted label = 0.806228350854, expected label = 1
------------------------------------------------------------------------
Classifying line:
  606	This is a great deal.	1
  words = [great, deal, great deal]
  predicted label = 0.970762517884, expected label = 1
------------------------------------------------------------------------
Classifying line:
  607	Excellent product, I am very satisfied with the purchase.	1
  words = [excel, product, satisfi, purchas, excel product, product satisfi, satisfi purchas]
  predicted label = 0.915523883372, expected label = 1
------------------------------------------------------------------------
Classifying line:
  608	I highly recommend these and encourage people to give them a try.	1
  words = [highli, recommend, encourag, peopl, give, tri, highli recommend, recommend encourag, encourag peopl, peopl give, give tri]
  predicted label = 0.981735456049, expected label = 1
------------------------------------------------------------------------
Classifying line:
  609	Better than you'd expect.	1
  words = [better, expect, better expect]
  predicted label = 0.199185127815, expected label = 1
------------------------------------------------------------------------
Classifying line:
  610	Great product and price.	1
  words = [great, product, price, great product, product price]
  predicted label = 0.988661236483, expected label = 1

================================
You guessed 324/600 = 54.0% correct.
  - False positive rate: [0.0, 0.0025, 0.0025, 0.005, 0.005, 0.0075, 0.0075, 0.01, 0.01, 0.0125, 0.0125, 0.0125, 0.0125, 0.015, 0.015, 0.0225, 0.0225, 0.025, 0.025, 0.03, 0.03, 0.0375, 0.0375, 0.04, 0.04, 0.0425, 0.0425, 0.0525, 0.0525, 0.0525, 0.0575, 0.0575, 0.065, 0.065, 0.0675, 0.0675, 0.07, 0.07, 0.0725, 0.0725, 0.0775, 0.0775, 0.0875, 0.0875, 0.0925, 0.0925, 0.0975, 0.0975, 0.1, 0.1, 0.11, 0.11, 0.11, 0.1125, 0.1125, 0.115, 0.115, 0.1175, 0.1175, 0.1225, 0.1225, 0.125, 0.125, 0.1275, 0.1275, 0.1325, 0.1325, 0.1375, 0.1375, 0.1425, 0.1425, 0.155, 0.155, 0.1575, 0.1575, 0.1575, 0.16, 0.16, 0.1625, 0.1625, 0.1675, 0.1675, 0.1775, 0.18, 0.18, 0.1825, 0.1825, 0.1825, 0.185, 0.185, 0.2075, 0.2075, 0.21, 0.21, 0.2125, 0.2125, 0.2175, 0.2175, 0.225, 0.2425, 0.2475, 0.2525, 0.2525, 0.2575, 0.26, 0.2675, 0.2675, 0.2775, 0.2775, 0.295, 0.295, 0.31, 0.31, 0.3125, 0.3125, 0.32, 0.32, 0.325, 0.335, 0.335, 0.3375, 0.3625, 0.3625, 0.365, 0.37, 0.3725, 0.3725, 0.375, 0.375, 0.3825, 0.3825, 0.385, 0.385, 0.39, 0.395, 0.4125, 0.4125, 0.4275, 0.4325, 0.4375, 0.4375, 0.44, 0.44, 0.445, 0.445, 0.4475, 0.4475, 0.45, 0.45, 0.4575, 0.4575, 0.4625, 0.535, 0.54, 0.54, 0.5425, 0.5425, 0.55, 0.55, 0.555, 0.555, 0.5725, 0.5725, 0.5925, 0.5925, 0.595, 0.6, 0.61, 0.6125, 0.6125, 0.62, 0.63, 0.63, 0.635, 0.635, 0.64, 0.645, 0.645, 0.65, 0.65, 0.6525, 0.6525, 0.66, 0.6625, 0.6625, 0.69, 0.69, 0.6975, 0.6975, 0.6975, 0.6975, 0.7075, 0.715, 0.72, 0.7275, 0.7275, 0.7325, 0.735, 0.74, 0.74, 0.7675, 0.7675, 0.79, 0.7925, 0.7975, 0.7975, 0.82, 0.82, 0.8225, 0.8225, 0.825, 0.825, 0.8325, 0.8325, 0.8375, 0.8375, 0.8425, 0.8425, 0.85, 0.85, 0.8525, 0.8525, 0.855, 0.86, 0.8625, 0.8625, 0.8675, 0.875, 0.8775, 0.8775, 0.8825, 0.8825, 0.8875, 0.8875, 0.8975, 0.8975, 0.905, 0.905, 0.91, 0.9125, 0.9125, 0.9325, 0.9325, 0.9625, 0.9625, 0.965, 0.965, 1.0]
  - True positive rate: [0.0, 0.0, 0.005, 0.005, 0.025, 0.025, 0.03, 0.03, 0.035, 0.035, 0.04, 0.06, 0.075, 0.075, 0.09, 0.09, 0.11, 0.11, 0.12, 0.12, 0.125, 0.125, 0.135, 0.135, 0.15, 0.15, 0.16, 0.16, 0.175, 0.185, 0.185, 0.19, 0.19, 0.195, 0.195, 0.2, 0.2, 0.215, 0.215, 0.22, 0.22, 0.235, 0.235, 0.25, 0.25, 0.255, 0.255, 0.26, 0.26, 0.27, 0.27, 0.28, 0.285, 0.285, 0.29, 0.29, 0.31, 0.31, 0.315, 0.315, 0.325, 0.325, 0.34, 0.34, 0.345, 0.345, 0.35, 0.35, 0.365, 0.365, 0.38, 0.38, 0.39, 0.39, 0.395, 0.41, 0.41, 0.42, 0.42, 0.43, 0.43, 0.44, 0.44, 0.44, 0.455, 0.455, 0.465, 0.47, 0.47, 0.475, 0.475, 0.48, 0.48, 0.49, 0.495, 0.5, 0.5, 0.51, 0.51, 0.51, 0.51, 0.51, 0.515, 0.515, 0.525, 0.525, 0.53, 0.53, 0.54, 0.54, 0.55, 0.55, 0.555, 0.555, 0.56, 0.56, 0.565, 0.57, 0.57, 0.575, 0.58, 0.58, 0.59, 0.59, 0.59, 0.59, 0.595, 0.595, 0.6, 0.6, 0.61, 0.61, 0.615, 0.62, 0.62, 0.62, 0.625, 0.625, 0.625, 0.625, 0.63, 0.63, 0.635, 0.635, 0.64, 0.64, 0.645, 0.645, 0.66, 0.66, 0.665, 0.665, 0.7, 0.7, 0.715, 0.715, 0.72, 0.72, 0.725, 0.725, 0.73, 0.73, 0.745, 0.745, 0.765, 0.765, 0.765, 0.765, 0.775, 0.78, 0.78, 0.78, 0.785, 0.785, 0.79, 0.79, 0.79, 0.795, 0.795, 0.805, 0.805, 0.815, 0.815, 0.815, 0.82, 0.82, 0.825, 0.825, 0.83, 0.84, 0.845, 0.845, 0.845, 0.845, 0.845, 0.85, 0.85, 0.855, 0.855, 0.86, 0.86, 0.87, 0.87, 0.875, 0.875, 0.885, 0.885, 0.89, 0.89, 0.895, 0.895, 0.9, 0.9, 0.905, 0.905, 0.91, 0.91, 0.92, 0.92, 0.93, 0.93, 0.935, 0.935, 0.935, 0.935, 0.94, 0.94, 0.94, 0.94, 0.945, 0.945, 0.955, 0.955, 0.96, 0.96, 0.965, 0.965, 0.97, 0.97, 0.97, 0.98, 0.98, 0.99, 0.99, 0.995, 0.995, 1.0, 1.0]
  - Thresholds: [1.9995721566360478, 0.9995721566360479, 0.9971761912514021, 0.9969044554761072, 0.995573594620517, 0.9948649535214048, 0.992060158082144, 0.9911142566829845, 0.9905267203741354, 0.9898967044281213, 0.9894824574028463, 0.9886612364830479, 0.987543886521868, 0.9874769622819536, 0.9848877499025561, 0.9835488331542658, 0.9789926583445124, 0.978624211859191, 0.9780342727687827, 0.9763066894284914, 0.9761184043474, 0.9758417007656733, 0.9754667975184953, 0.9747956946314926, 0.9707625178844037, 0.9707331496649233, 0.9668420919171732, 0.962045957309432, 0.960800238985646, 0.9596223484449405, 0.9594203696881486, 0.9575412718605955, 0.9564730141243749, 0.9546855933556638, 0.9537481678325452, 0.9534632623087357, 0.9518876740156946, 0.9512362264783892, 0.9483573348629183, 0.9474814330048787, 0.9465213867952068, 0.9449752815676536, 0.9413836259299506, 0.9376797153994382, 0.9372275009847525, 0.935073604457416, 0.934073719318592, 0.9323775854649766, 0.9322125587088811, 0.9277463511368294, 0.9206490951783098, 0.918838203440924, 0.9184292858416224, 0.9163994748036478, 0.915523883372141, 0.9144613867449808, 0.9114042212038822, 0.9108533741292187, 0.9086201784036938, 0.9080268297015213, 0.906931275040426, 0.9068278257422434, 0.8926006339188125, 0.891712261562591, 0.8899994129422305, 0.8872332817117692, 0.8857983042538142, 0.8830266548036677, 0.8812998916726965, 0.8778414007585337, 0.8772014314239807, 0.8730744419147759, 0.8671946610662654, 0.8637459036784759, 0.8620574064885843, 0.8609242686627298, 0.8582132557167659, 0.8562056289935515, 0.8548120927468513, 0.8508797349700107, 0.8483034871611898, 0.845083904180934, 0.8440593350792415, 0.8439892483370339, 0.8391471076317922, 0.8382310132074703, 0.8344950367317131, 0.8291993692624718, 0.8248577163560352, 0.8245955959270993, 0.8160160547162504, 0.8144361200427593, 0.8132642015706908, 0.8057787298249105, 0.804949779056689, 0.8049323157324585, 0.7895530062869893, 0.7837769861073172, 0.7837254738351861, 0.7667565139981775, 0.7581078666414856, 0.7561789529986711, 0.754140930945626, 0.7536830523961736, 0.7534568276840186, 0.7469516011431672, 0.744550172946045, 0.7392568321673174, 0.735935943862931, 0.7142491190639927, 0.7064406782899059, 0.6954391223095511, 0.6953913617550063, 0.6943632497333423, 0.6927075309875993, 0.685930458236242, 0.6855690335423574, 0.6854902405285013, 0.669136303112237, 0.6676003310425311, 0.6667380580117128, 0.6401131472315059, 0.6372043668112921, 0.6325763225702922, 0.6298720296893197, 0.6290456626585532, 0.6271645760417398, 0.626433357979602, 0.6198768274768712, 0.6077676294952162, 0.6065987034848268, 0.6058668040127815, 0.6048522838688698, 0.6044251965745996, 0.6040018171681989, 0.583799402249207, 0.5803157307275738, 0.5687056329421188, 0.5664164322249465, 0.5634009590300315, 0.5627343671872714, 0.5606716840468572, 0.5601665668749237, 0.5526287918425508, 0.5493668578033917, 0.5469909681975164, 0.5403209855092033, 0.5402188947933234, 0.5380480825494727, 0.5213691706015678, 0.5179855123266461, 0.5071147188283317, 0.506892600390953, 0.503561057870539, 0.49025571939125623, 0.4876338350710842, 0.4852612147883022, 0.4770541385631452, 0.4752607535756945, 0.4744602122080074, 0.47439865630862854, 0.458846775726299, 0.4481796042327732, 0.43076929307781153, 0.4213018459904595, 0.420929320884059, 0.41991164076747656, 0.4030546413472465, 0.40291670667838747, 0.3988796374397073, 0.3945528162494944, 0.3816335445535317, 0.3765345698280046, 0.369428425759657, 0.36640175851931966, 0.3645289380931909, 0.36262548820046736, 0.3625183537442533, 0.3577439803898989, 0.34833601742074843, 0.3481643086819973, 0.34473290029691916, 0.3424783984737136, 0.33887962188265813, 0.33792867197749565, 0.3167411125691413, 0.31267629880946846, 0.3025532776075224, 0.29963049015016546, 0.29931018339766385, 0.29894578771964647, 0.29365699609237506, 0.2923790730730698, 0.2921041517877735, 0.2902300054412096, 0.28715064625371517, 0.2845570151236924, 0.2821454182310243, 0.27940285091233896, 0.2746061617778465, 0.2452306364335481, 0.2414423333232433, 0.2144921371182103, 0.2058729031733709, 0.2032178624233485, 0.19918512781549008, 0.1661536711873698, 0.16583811613153304, 0.164349160998523, 0.16147418635930236, 0.1548931443100324, 0.15257524993232718, 0.1449034176025682, 0.14133535209499312, 0.1337942424951655, 0.13375840488465984, 0.1275955362712292, 0.12634085682551213, 0.11793382071754971, 0.11521073966072995, 0.11185999384007203, 0.11164818112945174, 0.11035506567572742, 0.10945100478142726, 0.10713751474499454, 0.1057599654779745, 0.10497312569661993, 0.10491243747098022, 0.10403567332134622, 0.10397183630865668, 0.10294588177305367, 0.10112127059453145, 0.09482927646546888, 0.09321829575381252, 0.08722041711796402, 0.0864112928181538, 0.08433159802222831, 0.08105873705920849, 0.07942865793100608, 0.07747811041209536, 0.07082154901193062, 0.05040282486488819, 0.04681510067451015, 0.02442511501479201, 0.023177149921865845, 0.021399380667337993, 0.02041644758446738, 0.0002711128570598583]
  - AUC: 0.65779375
  - Precision: 0.393258426966
  - Recall: 0.7
  - F1: 0.503597122302
================================

Running model 'mnb' on train data 'default.txt2.train' and test data 'default.txt2.test'
------------------------------------------------------------------------
Classifying line:
  601	Magical Help.	1
  words = [magic, help, magic help]
  predicted label = 0.571906360526, expected label = 1
------------------------------------------------------------------------
Classifying line:
  602	The best phone in market :).	1
  words = [best, phone, market, best phone, phone market]
  predicted label = 0.880905302215, expected label = 1
------------------------------------------------------------------------
Classifying line:
  603	It worked very well.	1
  words = [work, well, work well]
  predicted label = 0.873674243294, expected label = 1
------------------------------------------------------------------------
Classifying line:
  604	The company shipped my product very promptly and it works well.	1
  words = [compani, ship, product, promptli, work, well, compani ship, ship product, product promptli, promptli work, work well]
  predicted label = 0.545945469811, expected label = 1
------------------------------------------------------------------------
Classifying line:
  605	Exactly what I wanted.	1
  words = [exactli, want, exactli want]
  predicted label = 0.804277183712, expected label = 1
------------------------------------------------------------------------
Classifying line:
  606	This is a great deal.	1
  words = [great, deal, great deal]
  predicted label = 0.970701087793, expected label = 1
------------------------------------------------------------------------
Classifying line:
  607	Excellent product, I am very satisfied with the purchase.	1
  words = [excel, product, satisfi, purchas, excel product, product satisfi, satisfi purchas]
  predicted label = 0.921330515649, expected label = 1
------------------------------------------------------------------------
Classifying line:
  608	I highly recommend these and encourage people to give them a try.	1
  words = [highli, recommend, encourag, peopl, give, tri, highli recommend, recommend encourag, encourag peopl, peopl give, give tri]
  predicted label = 0.979778916657, expected label = 1
------------------------------------------------------------------------
Classifying line:
  609	Better than you'd expect.	1
  words = [better, expect, better expect]
  predicted label = 0.22859822349, expected label = 1
------------------------------------------------------------------------
Classifying line:
  610	Great product and price.	1
  words = [great, product, price, great product, product price]
  predicted label = 0.988030577266, expected label = 1

================================
You guessed 333/600 = 55.5% correct.
  - False positive rate: [0.0, 0.0025, 0.0025, 0.0075, 0.0075, 0.01, 0.01, 0.0125, 0.0125, 0.0175, 0.0175, 0.0175, 0.02, 0.02, 0.025, 0.025, 0.03, 0.03, 0.035, 0.035, 0.035, 0.035, 0.04, 0.04, 0.05, 0.05, 0.055, 0.055, 0.0575, 0.0575, 0.06, 0.06, 0.0725, 0.0725, 0.075, 0.075, 0.0775, 0.0775, 0.08, 0.08, 0.085, 0.085, 0.0875, 0.0875, 0.0925, 0.0925, 0.095, 0.095, 0.1025, 0.1025, 0.1025, 0.1075, 0.1075, 0.11, 0.11, 0.115, 0.115, 0.12, 0.12, 0.1225, 0.1225, 0.13, 0.13, 0.1325, 0.1325, 0.135, 0.135, 0.1375, 0.1375, 0.1475, 0.1475, 0.1525, 0.1525, 0.155, 0.155, 0.155, 0.165, 0.175, 0.175, 0.1775, 0.1775, 0.18, 0.18, 0.185, 0.185, 0.185, 0.185, 0.1975, 0.1975, 0.2, 0.2025, 0.22, 0.22, 0.2275, 0.2275, 0.23, 0.235, 0.2375, 0.2375, 0.2425, 0.2475, 0.25, 0.2575, 0.2575, 0.26, 0.26, 0.27, 0.27, 0.2875, 0.2875, 0.3025, 0.3025, 0.305, 0.305, 0.3125, 0.315, 0.315, 0.325, 0.3275, 0.3425, 0.3425, 0.3525, 0.3525, 0.355, 0.36, 0.365, 0.365, 0.37, 0.37, 0.37, 0.375, 0.385, 0.3975, 0.4025, 0.4025, 0.41, 0.41, 0.415, 0.4175, 0.4175, 0.4225, 0.4225, 0.425, 0.4975, 0.5025, 0.5075, 0.5075, 0.51, 0.51, 0.52, 0.52, 0.525, 0.525, 0.53, 0.53, 0.545, 0.545, 0.5475, 0.5475, 0.555, 0.555, 0.5625, 0.5625, 0.5725, 0.5775, 0.585, 0.585, 0.59, 0.5925, 0.5925, 0.6, 0.6025, 0.6025, 0.6225, 0.6225, 0.625, 0.625, 0.6275, 0.635, 0.64, 0.64, 0.6425, 0.6425, 0.65, 0.65, 0.6525, 0.6525, 0.6725, 0.6725, 0.675, 0.6825, 0.6875, 0.6925, 0.6975, 0.7, 0.7025, 0.7025, 0.7075, 0.715, 0.7175, 0.7175, 0.725, 0.725, 0.735, 0.735, 0.76, 0.76, 0.7625, 0.7625, 0.7625, 0.77, 0.77, 0.775, 0.775, 0.7875, 0.7875, 0.795, 0.795, 0.805, 0.805, 0.8075, 0.8075, 0.825, 0.825, 0.8275, 0.8275, 0.8325, 0.8325, 0.845, 0.845, 0.8475, 0.855, 0.8575, 0.8575, 0.8675, 0.8675, 0.8725, 0.8725, 0.8775, 0.8825, 0.8825, 0.885, 0.885, 0.9175, 0.9175, 0.9225, 0.9225, 0.925, 0.925, 0.9375, 0.9375, 0.945, 0.945, 0.955, 0.955, 1.0]
  - True positive rate: [0.005, 0.005, 0.015, 0.015, 0.02, 0.02, 0.03, 0.03, 0.035, 0.035, 0.055, 0.065, 0.065, 0.09, 0.09, 0.095, 0.095, 0.11, 0.11, 0.115, 0.125, 0.14, 0.14, 0.155, 0.155, 0.17, 0.17, 0.175, 0.175, 0.18, 0.18, 0.195, 0.195, 0.2, 0.2, 0.21, 0.21, 0.22, 0.22, 0.23, 0.23, 0.235, 0.235, 0.24, 0.24, 0.25, 0.25, 0.26, 0.26, 0.27, 0.28, 0.28, 0.285, 0.285, 0.295, 0.295, 0.305, 0.305, 0.325, 0.325, 0.335, 0.335, 0.345, 0.345, 0.355, 0.355, 0.365, 0.365, 0.37, 0.37, 0.375, 0.375, 0.39, 0.39, 0.405, 0.415, 0.415, 0.415, 0.425, 0.425, 0.435, 0.435, 0.44, 0.44, 0.445, 0.455, 0.465, 0.465, 0.475, 0.475, 0.48, 0.48, 0.485, 0.485, 0.49, 0.49, 0.49, 0.49, 0.5, 0.5, 0.5, 0.51, 0.51, 0.525, 0.525, 0.53, 0.53, 0.535, 0.535, 0.54, 0.54, 0.55, 0.55, 0.555, 0.56, 0.56, 0.57, 0.57, 0.575, 0.575, 0.585, 0.585, 0.59, 0.59, 0.59, 0.59, 0.595, 0.595, 0.605, 0.615, 0.615, 0.62, 0.62, 0.62, 0.625, 0.625, 0.63, 0.63, 0.63, 0.64, 0.64, 0.655, 0.655, 0.69, 0.69, 0.69, 0.705, 0.705, 0.71, 0.71, 0.715, 0.715, 0.72, 0.72, 0.725, 0.725, 0.73, 0.73, 0.745, 0.745, 0.75, 0.75, 0.755, 0.755, 0.755, 0.755, 0.76, 0.76, 0.765, 0.77, 0.77, 0.77, 0.775, 0.775, 0.78, 0.78, 0.785, 0.785, 0.785, 0.785, 0.79, 0.79, 0.8, 0.8, 0.805, 0.805, 0.815, 0.815, 0.82, 0.82, 0.82, 0.82, 0.82, 0.82, 0.825, 0.825, 0.84, 0.84, 0.84, 0.84, 0.845, 0.845, 0.85, 0.85, 0.855, 0.855, 0.86, 0.86, 0.865, 0.875, 0.875, 0.885, 0.885, 0.89, 0.89, 0.895, 0.895, 0.9, 0.9, 0.905, 0.905, 0.91, 0.91, 0.915, 0.915, 0.925, 0.925, 0.935, 0.935, 0.94, 0.94, 0.94, 0.94, 0.945, 0.945, 0.955, 0.955, 0.96, 0.96, 0.96, 0.965, 0.965, 0.97, 0.97, 0.975, 0.975, 0.98, 0.98, 0.985, 0.985, 0.99, 0.99, 0.995, 0.995, 1.0, 1.0]
  - Thresholds: [0.9997866942738922, 0.9992504684367443, 0.9966643008903489, 0.99638681817464, 0.9963626403509301, 0.9963211267189728, 0.9950430420600878, 0.9938755478230684, 0.9920128743058901, 0.9893473533334232, 0.9880305772655373, 0.9869516084816162, 0.9853256769391044, 0.9829812263133703, 0.9820012768526389, 0.9810211152922405, 0.9804970542409999, 0.9781241620801501, 0.9766982458074203, 0.9764227124589006, 0.9754644289036346, 0.9740265132300423, 0.9724818084876645, 0.9707010877930613, 0.9658163415926424, 0.9649420272531729, 0.9624972165800595, 0.9616975510424048, 0.9606742235337179, 0.9604991956163671, 0.9596018516932092, 0.9549314150210111, 0.9522669962756833, 0.9522404930490056, 0.952147492834803, 0.9484385388427619, 0.9481097674707076, 0.9476384644346333, 0.9460293504137243, 0.94442950111768, 0.94298803898847, 0.9423996233267284, 0.94185985559056, 0.9413512214616693, 0.9401746419743069, 0.9361803239791557, 0.9344808249197828, 0.93313031812495, 0.9324953746890549, 0.9297125160352404, 0.923588608248001, 0.9229106458667883, 0.9225832946904055, 0.9221333421095171, 0.9196844765355136, 0.9147818628412909, 0.9140762357287621, 0.9130965980177284, 0.9035034018536346, 0.9015783593958872, 0.8977876842569337, 0.8957048096499486, 0.8921180190089195, 0.8917741320528186, 0.8902051897625909, 0.8891378955408152, 0.8883105839616297, 0.887597900650506, 0.887102398035397, 0.8828169170138539, 0.8809053022151062, 0.8789075647458777, 0.8756037073803662, 0.8752813244636617, 0.8736742432942828, 0.8727616559961922, 0.8619379470398575, 0.8575307645150196, 0.8568240185971284, 0.8528625913648175, 0.8513109888360737, 0.8485498273031725, 0.8481197450978625, 0.8422506173549057, 0.8400480715769582, 0.8397259853261755, 0.8340800960822409, 0.8269059506768802, 0.8263108682016872, 0.821968953576083, 0.8218156543854783, 0.8087105992833181, 0.804277183711574, 0.8032282898141024, 0.799264868425662, 0.7975607962261895, 0.7924003747521625, 0.7917462531541317, 0.7853250044259604, 0.7762847327908625, 0.7736155335540252, 0.7695769620491936, 0.7639308676071074, 0.7551317984120851, 0.7550190332017057, 0.7515653227224546, 0.7411152893100899, 0.7405398402161659, 0.7304575170995699, 0.728189958686756, 0.713341658936351, 0.7092170585574677, 0.7084547130574655, 0.7048048648952072, 0.7046316187571103, 0.7033496026022743, 0.6957177158726849, 0.6889102109849774, 0.6856455546097849, 0.6578753483524179, 0.6561173765198982, 0.6522135864529686, 0.6513065265414935, 0.651124867489719, 0.6489801970668836, 0.6449780668660753, 0.6433848340676366, 0.6425768247882709, 0.6392707251644169, 0.636289837221997, 0.6305772706295907, 0.625457211310735, 0.5916450809401688, 0.5887078324304489, 0.5793245721854344, 0.5729824085010798, 0.5719063605256351, 0.5709336797210783, 0.5595020844560337, 0.5585851828180479, 0.5544142031882757, 0.5459454698113168, 0.5451518688243641, 0.5416666666666667, 0.54050051025748, 0.5361202650960561, 0.5343987598526299, 0.5291076277786395, 0.525954448016169, 0.5210311338750009, 0.5161669815488039, 0.5123349905020141, 0.5119523185266406, 0.5027206951451935, 0.5011573100877713, 0.4794161081794526, 0.4792997295793733, 0.4761751368794225, 0.4698000821961148, 0.46658729588537784, 0.4652530384588087, 0.45907934815061413, 0.4589372041637592, 0.4434073161207162, 0.4429592848888537, 0.44212037006740884, 0.432911398432088, 0.42907240072147157, 0.4260089746129831, 0.4239929047801713, 0.41714100450531766, 0.41670402690925695, 0.4156213221375592, 0.39352917873048116, 0.3931049203862937, 0.385711648366728, 0.38133740681254147, 0.3800931128195405, 0.37791590775645306, 0.3747139355252411, 0.3699697158046473, 0.3695046150259696, 0.3625676476198806, 0.35037144840548773, 0.3472206444522819, 0.3464624417367554, 0.3457650021834843, 0.32584065011056623, 0.3244477975291827, 0.3244180988648046, 0.3230071833057419, 0.3076567459049037, 0.30643117338303627, 0.305587700078625, 0.30252636566421265, 0.302357633578458, 0.29702879409879385, 0.2933438536700024, 0.2917830758878531, 0.2907965139785683, 0.2896852724133223, 0.28460456153977526, 0.2806768209436359, 0.2699935634336146, 0.2688002727681067, 0.24301860696244706, 0.2421408525037935, 0.23055260468644798, 0.23039343900294698, 0.22859822349046954, 0.2241643572491331, 0.22255965717624066, 0.21423202618229129, 0.20805790668835547, 0.19261095222963687, 0.18820625753344597, 0.18042142775824427, 0.17930976006645374, 0.16778490638580187, 0.16602580798684624, 0.1597286290536362, 0.15490772079717566, 0.13213967922148578, 0.131051788871366, 0.13050158418508842, 0.12364846313099623, 0.11648208499521154, 0.11504189506327522, 0.10880721080366135, 0.10867346172104471, 0.10718379760142441, 0.10656861756866688, 0.10557751131722747, 0.10539086166606848, 0.09587593828807102, 0.09348509299524886, 0.08785001364509848, 0.08739801179634753, 0.08711156588505592, 0.08559189862608237, 0.08469552966227063, 0.08432622739956457, 0.07836606787725041, 0.04902794117100403, 0.04776310027409922, 0.044764158603463354, 0.04319931288834804, 0.03839239509216681, 0.03709227157140396, 0.02813109315171246, 0.027909421503115255, 0.02389384907770943, 0.02244073098572505, 0.01981814638207718, 0.019344299077687924, 0.00012922626925942734]
  - AUC: 0.661625
  - Precision: 0.406162464986
  - Recall: 0.725
  - F1: 0.520646319569
================================

Running model 'dt' on train data 'default.txt2.train' and test data 'default.txt2.test'
Best hyperparameters were: {'max_features': 0.9, 'min_samples_leaf': 1}
------------------------------------------------------------------------
Classifying line:
  601	Magical Help.	1
  words = [magic, help, magic help]
  predicted label = 0.0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  602	The best phone in market :).	1
  words = [best, phone, market, best phone, phone market]
  predicted label = 1.0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  603	It worked very well.	1
  words = [work, well, work well]
  predicted label = 1.0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  604	The company shipped my product very promptly and it works well.	1
  words = [compani, ship, product, promptli, work, well, compani ship, ship product, product promptli, promptli work, work well]
  predicted label = 1.0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  605	Exactly what I wanted.	1
  words = [exactli, want, exactli want]
  predicted label = 1.0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  606	This is a great deal.	1
  words = [great, deal, great deal]
  predicted label = 1.0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  607	Excellent product, I am very satisfied with the purchase.	1
  words = [excel, product, satisfi, purchas, excel product, product satisfi, satisfi purchas]
  predicted label = 1.0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  608	I highly recommend these and encourage people to give them a try.	1
  words = [highli, recommend, encourag, peopl, give, tri, highli recommend, recommend encourag, encourag peopl, peopl give, give tri]
  predicted label = 1.0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  609	Better than you'd expect.	1
  words = [better, expect, better expect]
  predicted label = 0.0, expected label = 1
------------------------------------------------------------------------
Classifying line:
  610	Great product and price.	1
  words = [great, product, price, great product, product price]
  predicted label = 1.0, expected label = 1

================================
You guessed 326/600 = 54.3333333333% correct.
  - False positive rate: [0.0, 0.44, 0.44, 0.4475, 0.4575, 0.53, 0.53, 0.5475, 0.645, 0.6625, 0.665, 0.7275, 1.0]
  - True positive rate: [0.0, 0.57, 0.59, 0.6, 0.63, 0.655, 0.66, 0.725, 0.77, 0.78, 0.78, 0.78, 1.0]
  - Thresholds: [2.0, 1.0, 0.875, 0.75, 0.7, 0.6666666666666666, 0.5714285714285714, 0.5, 0.4409090909090909, 0.3333333333333333, 0.2, 0.16666666666666666, 0.0]
  - AUC: 0.57438125
  - Precision: 0.398351648352
  - Recall: 0.725
  - F1: 0.514184397163
================================

Running model 'rf' on train data 'default.txt2.train' and test data 'default.txt2.test'
------------------------------------------------------------------------
Classifying line:
  601	Magical Help.	1
  words = [magic, help, magic help]
  predicted label = 0.477837887267, expected label = 1
------------------------------------------------------------------------
Classifying line:
  602	The best phone in market :).	1
  words = [best, phone, market, best phone, phone market]
  predicted label = 0.931708035871, expected label = 1
------------------------------------------------------------------------
Classifying line:
  603	It worked very well.	1
  words = [work, well, work well]
  predicted label = 0.7222557066, expected label = 1
------------------------------------------------------------------------
Classifying line:
  604	The company shipped my product very promptly and it works well.	1
  words = [compani, ship, product, promptli, work, well, compani ship, ship product, product promptli, promptli work, work well]
  predicted label = 0.691536893135, expected label = 1
------------------------------------------------------------------------
Classifying line:
  605	Exactly what I wanted.	1
  words = [exactli, want, exactli want]
  predicted label = 0.476638408779, expected label = 1
------------------------------------------------------------------------
Classifying line:
  606	This is a great deal.	1
  words = [great, deal, great deal]
  predicted label = 0.991112529619, expected label = 1
------------------------------------------------------------------------
Classifying line:
  607	Excellent product, I am very satisfied with the purchase.	1
  words = [excel, product, satisfi, purchas, excel product, product satisfi, satisfi purchas]
  predicted label = 0.964906678316, expected label = 1
------------------------------------------------------------------------
Classifying line:
  608	I highly recommend these and encourage people to give them a try.	1
  words = [highli, recommend, encourag, peopl, give, tri, highli recommend, recommend encourag, encourag peopl, peopl give, give tri]
  predicted label = 0.604176630167, expected label = 1
------------------------------------------------------------------------
Classifying line:
  609	Better than you'd expect.	1
  words = [better, expect, better expect]
  predicted label = 0.399464262558, expected label = 1
------------------------------------------------------------------------
Classifying line:
  610	Great product and price.	1
  words = [great, product, price, great product, product price]
  predicted label = 0.991112529619, expected label = 1

================================
You guessed 323/600 = 53.8333333333% correct.
  - False positive rate: [0.0, 0.0075, 0.0075, 0.01, 0.01, 0.01, 0.01, 0.0125, 0.0125, 0.025, 0.0275, 0.0275, 0.03, 0.03, 0.03, 0.03, 0.0375, 0.05, 0.0575, 0.135, 0.15, 0.16, 0.1675, 0.1675, 0.1725, 0.185, 0.19, 0.195, 0.2, 0.2025, 0.225, 0.2375, 0.245, 0.2475, 0.2475, 0.2925, 0.3275, 0.3275, 0.3475, 0.3475, 0.3525, 0.3525, 0.355, 0.355, 0.3625, 0.365, 0.3675, 0.3675, 0.37, 0.37, 0.375, 0.385, 0.385, 0.385, 0.3875, 0.3875, 0.39, 0.39, 0.3925, 0.3975, 0.4025, 0.4025, 0.405, 0.41, 0.42, 0.4225, 0.4225, 0.4275, 0.4275, 0.435, 0.435, 0.44, 0.4425, 0.4425, 0.445, 0.4475, 0.4475, 0.45, 0.45, 0.4525, 0.455, 0.46, 0.46, 0.465, 0.47, 0.475, 0.475, 0.4825, 0.4825, 0.485, 0.4925, 0.495, 0.4975, 0.5, 0.505, 0.5075, 0.5125, 0.5125, 0.7275, 0.7275, 0.73, 0.73, 0.73, 0.7325, 0.7325, 0.735, 0.74, 0.7425, 0.7425, 0.7475, 0.7575, 0.7575, 0.7625, 0.7625, 0.765, 0.765, 0.77, 0.775, 0.7775, 0.785, 0.7875, 0.7875, 0.79, 0.79, 0.7925, 0.7925, 0.795, 0.795, 0.8025, 0.8025, 0.8075, 0.8075, 0.8075, 0.81, 0.81, 0.8125, 0.8175, 0.82, 0.8275, 0.8325, 0.8325, 0.835, 0.845, 0.845, 0.85, 0.8525, 0.8525, 0.8575, 0.8625, 0.8725, 0.8725, 0.875, 0.875, 0.8825, 0.8825, 0.885, 0.885, 0.8875, 0.8875, 0.89, 0.89, 0.8925, 0.8925, 0.895, 0.895, 0.895, 0.9175, 0.9475, 0.9525, 0.9525, 0.965, 0.98, 1.0]
  - True positive rate: [0.0, 0.16, 0.165, 0.165, 0.18, 0.185, 0.21, 0.21, 0.22, 0.25, 0.25, 0.26, 0.26, 0.27, 0.29, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.305, 0.305, 0.305, 0.305, 0.305, 0.305, 0.315, 0.385, 0.385, 0.385, 0.385, 0.39, 0.39, 0.39, 0.405, 0.405, 0.41, 0.41, 0.42, 0.42, 0.425, 0.425, 0.435, 0.435, 0.445, 0.445, 0.45, 0.45, 0.45, 0.46, 0.465, 0.465, 0.475, 0.475, 0.48, 0.48, 0.48, 0.48, 0.485, 0.485, 0.495, 0.495, 0.495, 0.505, 0.505, 0.515, 0.515, 0.52, 0.52, 0.52, 0.525, 0.53, 0.53, 0.535, 0.535, 0.54, 0.54, 0.545, 0.545, 0.555, 0.555, 0.555, 0.565, 0.57, 0.57, 0.575, 0.575, 0.58, 0.58, 0.585, 0.585, 0.585, 0.585, 0.585, 0.59, 0.685, 0.695, 0.7, 0.71, 0.715, 0.715, 0.72, 0.72, 0.72, 0.72, 0.73, 0.73, 0.73, 0.735, 0.735, 0.74, 0.74, 0.755, 0.755, 0.755, 0.76, 0.76, 0.76, 0.765, 0.77, 0.78, 0.78, 0.785, 0.785, 0.79, 0.79, 0.795, 0.795, 0.805, 0.815, 0.815, 0.825, 0.825, 0.825, 0.825, 0.825, 0.825, 0.835, 0.84, 0.84, 0.845, 0.845, 0.845, 0.85, 0.85, 0.85, 0.85, 0.855, 0.875, 0.885, 0.885, 0.895, 0.895, 0.915, 0.915, 0.92, 0.92, 0.925, 0.925, 0.935, 0.935, 0.975, 0.995, 0.995, 0.995, 0.995, 1.0, 1.0, 1.0, 1.0]
  - Thresholds: [1.9911125296186032, 0.9911125296186032, 0.9815794577596384, 0.9715766367089238, 0.9649066783159224, 0.96260480751321, 0.960410356678837, 0.9588230804640032, 0.9552912937005377, 0.9536401556887878, 0.9519639228074577, 0.9497965100129772, 0.9494200953893766, 0.9323106935409418, 0.9317080358707988, 0.9172767089752716, 0.8439866113662093, 0.8430633633128166, 0.8389172431755826, 0.8384687933982959, 0.8001644616385528, 0.8001189555560502, 0.7923714663164522, 0.7905809669017045, 0.7875310293774668, 0.7865416953245716, 0.7791088318649768, 0.7766665941634235, 0.7710757159636199, 0.7616890954989483, 0.7590116761441095, 0.7505231078098876, 0.7499812751411814, 0.7488939203249096, 0.7483161149338042, 0.7480487114968186, 0.724653020546201, 0.7222557065995316, 0.7007157092377416, 0.6973664837022021, 0.6958528050640702, 0.6915368931351977, 0.689206863989877, 0.6879391323714179, 0.6725288529536476, 0.6685588746062615, 0.6672014034887124, 0.6584869229877818, 0.6483017660288385, 0.6482406248513038, 0.645388563110727, 0.6338671838269677, 0.6281357399768579, 0.6277282073121677, 0.6117554013786497, 0.6095898078812855, 0.6069996353705596, 0.6041766301670544, 0.6020420061863806, 0.6004551986292607, 0.5904424292451055, 0.5882186806392494, 0.581337145369241, 0.5805959145063148, 0.5789878481588407, 0.5770485743591017, 0.5769054383158386, 0.5733330862540789, 0.5676605964207265, 0.5560739292722698, 0.5549603071561146, 0.552634388867643, 0.552321945325682, 0.5508422453023154, 0.5478688202127605, 0.5448384063331617, 0.543228680215513, 0.5345156013375336, 0.531095973080482, 0.5293922595151029, 0.5253770865922763, 0.5138305795576558, 0.5126587232553458, 0.5094024308499069, 0.5072223117456873, 0.5058558870761273, 0.5047292823317366, 0.4908526145746274, 0.49084419116833966, 0.48826605557934555, 0.4877991057914507, 0.4810427481428902, 0.48089423934356296, 0.4808716710506087, 0.47987448233444624, 0.47891777572049343, 0.47862000525016846, 0.47852352918046853, 0.47783788726682497, 0.4766384087792109, 0.4751983500230661, 0.47493704902890804, 0.4739414673850037, 0.4737071432065034, 0.47338327359362664, 0.4733396222602998, 0.4728637940025762, 0.47102154184752615, 0.4653206921751649, 0.46495620392552867, 0.4475579450394289, 0.4458379867916601, 0.4410804377064826, 0.4386709860536236, 0.4384721733413859, 0.43189729600460436, 0.4291281594279561, 0.42912755260961655, 0.4252224088725827, 0.4235263232410137, 0.4230241510936334, 0.42255855585531354, 0.418269806729357, 0.4126384568441592, 0.41255507327366286, 0.4125039457241634, 0.41198767163839917, 0.40533508478131, 0.40300502370569524, 0.4024780201299165, 0.39961515140382553, 0.399464262557776, 0.39636101015908587, 0.3940722668705386, 0.39385399483387756, 0.39285802695013844, 0.38979256980299815, 0.38939447764491336, 0.3820746840795081, 0.37081653043173324, 0.36254391292083843, 0.3584313630073385, 0.3493005184715647, 0.3473901146800612, 0.3461441672603229, 0.34216408004437154, 0.3419546548046363, 0.33595690019517427, 0.3347553412983338, 0.3193247001613948, 0.3192549420008817, 0.3192381841530464, 0.3128276147871341, 0.2977284344386108, 0.2945479843763758, 0.29395843845679875, 0.2772084857150325, 0.2720142554218566, 0.2662967641320863, 0.2582609826925062, 0.2564185312273516, 0.25350571164183844, 0.24514171170244642, 0.24464817094381291, 0.2409229617024464, 0.2269119467535609, 0.13025492367499036, 0.1291129668633801, 0.12340658082211176, 0.1202395821707265, 0.06760403083083742, 0.06679153083083741, 0.004851013110846246]
  - AUC: 0.574275
  - Precision: 0.373770491803
  - Recall: 0.57
  - F1: 0.451485148515
================================

