\documentclass{article} % For LaTeX2e
\usepackage{cos424,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsmath}
%\usepackage{natbib}
\usepackage{multirow}
\usepackage{bm,bbm}
 \usepackage{amssymb}
 \usepackage{float}
 \title{Classification of Review Sentiments}

\author{
Andrew Or\\
Department of Computer Science\\
Princeton University\\
\texttt{andrewor@cs.princeton.edu} \\
%\And
%Coauthor \\
%Affiliation \\
%\texttt{email} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}

Customer sentiment is a good prediction of business value. Amazon and many other e-commerce websites rely on reviews to organically filter out unwanted or defective products. While the rating associated with each review is important, the content of the review is also a significant indication of customer interest or lack thereof. In this paper, we analyze sentiments expressed in the text of each review and explore the relationship between these sentiments and the associated rating. We use techniques in text analysis to extract and select features from the dataset and fit the data to a number of classification models. We evaluate these methods on 10,000 biased reviews taken from the Multi-Domain Sentiment Dataset [], which comprises of product reviews collected from Amazon.com across many product types.

\end{abstract}
\section{Introduction}

Sentiment analysis is a common technique used by many companies to gain insight into their users. In the context of e-commerce, sentiments are usually hidden in the product reviews. In Amazon's case, for instance, each review is associated with a five-star rating and a text blob justifying the user's decision on the rating. Other customers then use average ratings to judge whether their purchases will be fruitful.

The problem with analyzing ratings alone is that ratings are highly subjective. A product with mediocre ratings may still be desirable if, for example, the product has different sizes and many of the low ratings were given because the product did not fit the buyer. By contrast, the text associated with each review contains much more information than just a number. Certain words, such as 'amazing' and 'brilliant', are often strongly correlated with positive feedback. In addition to tokenized words, n-grams are often also of interest because the sentiments associated with certain phrases, such as 'not impressed', are different from those associated with each individual word in the phrase.

In this paper, we model the reviews using a bag-of-words representation and evaluate the accuracy of four different classification methods: Bernoulli naive Bayes, multinomial naive Bayes, decision trees, and random forest. We extracted bigrams in addition to individual word tokens as our features and selected features with low Chi-squared score. To account for potential skew in the input data, we used $k$-fold cross validation to split the data into different permutations of train and test sets. The metrics we used for evaluation include percentage of correctly classified test labels, area under the ROC curve, and the F1 score.

\section{Dataset}

We used two datasets in this experiment. The first is the Sentiment Labelled Sentences Data Set [], which consists of 3,000 labelled sentences extracted from reviews from IMDB, Amazon, and Yelp. There are 500 positive and 500 negative sentences from each of the websites and no neutral sentences. This dataset was used for evaluating a classification method that works around the lack of instance labels by exploring instance-level similarity [].

The second dataset used in this experiment is the Multi-Domain Sentient Dataset [], which contains 38548 Amazon biased reviews. The reviews were left by customers across many product categories, ranging from books and dvds, which have hundreds of thousands of reviews, to musical instruments, which have only a few hundred. The subset used in this project is comprised of 10,000 reviews randomly sampled from all categories. Prior work has used this dataset for a variety of purpose, ranging from sentiment classification [] to providing bounds for minimizing empirical risk [] to weighting linear classifiers with confidence [].

The reasons for using this second dataset are twofold. First, it is much larger than the first one, which leads to a more general model and thus lower generalization error. Many commonly used text analysis techniques we explored did not have a significant impact on the accuracy when training on the smaller dataset. Second, the Amazon dataset comes with ratings (out of 5 stars) in addition to just positive or negative labels. This enables us to generalize the techniques used in this project to multi-class classification.

\section{Data processing}

Raw word tokens are not suitable as features for several reasons. First, some words are used only very rarely and do not actually convey any sentiment. These could be words that are highly specific to the user or simply typos. Second, many words have multiple forms that are semantically equivalent. Third, analyzing individual word tokens misses semantics expressed through phrases or negation. Thus, feature extraction and selection are both conducive to a feature set more representative of the underlying sentiments expressed by the users.

\subsection{Feature extraction}

We used the NLTK library to tokenize, lower case, lemmatize and stem each word using the Porter stemming methods. In addition to individual word tokens, we also include bigrams in our bag of "words" representation. Each entry in our feature vector then is the number of times each "word" appears in a document.

\subsection{Feature selection}

Without any feature selection, the number of features extracted this way is 11252 for the smaller dataset and 166566 for the larger dataset. Especially in the latter case, classifying on this many features is prohibitively expensive and prone to noise.

Thus, we filtered the number of features as follows. First, we removed stop words, as specified by the NLTK library, and any words that occurred fewer than 5 times in the corpus. This step alone reduced the number of features to 515 in the smaller dataset and 6666 in the larger dataset. Then, we ranked the remaining features by Chi-squared score, which is given by:

\begin{align*}
\chi^2 = \sum_{ij}{\dfrac{(O_{ij} - E_{ij})^2}{E_{ij}}}
\end{align*}

where $O_{ij}$ is the observed count and $E_{ij}$ is the expected count. This quantity measures how much observed counts deviate from expected counts and tests the independence between two variables: the feature variable and the class variable in our case. We then select the best $k$ (lowest $\chi^2$) features and use them for classification.

We experimented with different values of $k$ depending on the dataset. For the smaller dataset, we used $k = 463$, which throws approximately 10\% of the remaining features after removing rare words and stop words. For the larger dataset, we used $k = 4000$, which throws about around 40\% of the remaining features.

\section{Classification}

We trained four different models using this set of features: Bernoulli naive Bayes, multionmial naive Bayes, decision trees, and random forests. In this section, we will discuss random forests in detail.

Random forest [] is an ensemble learning method used for classification and other machine learning tasks. The central idea behind random forest is bootstrap aggregating, also known as bagging, a technique that combines the results of multiple learning algorithms to reduce variance and avoid overfitting.

The algorithm generates $B$ random subsets of the input data and trains a decision tree on each one of the subsets. Additionally, in the spirit of the random subspace method [], each decision tree chooses $d$ features sampled randomly with replacement from the original set of features. Then, to predict a data point, the algorithm traverses all $B$ decision trees and returns the mode as the predicted label.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{dt.png}
  \caption{A complex decision tree, borrowed from []}
\end{figure}

Each decision tree can be built as follows using the ID3 algorithm []:
\begin{enumerate}
\item Select the best feature that minimizes entropy (or maximizes information gain)
\item Split input data by the values of the feature, creating a node for each value
\item Recurse on each node by selecting the best remaining features
\end{enumerate}

Entropy is defined as follows:
\begin{align*}
\mathbb{H}(\pi) = -\sum_{c=1}^{C}{\pi_c log(\pi_c)}\\
\pi_c = \dfrac{1}{|\mathcal{D}|}\sum_{i \in \mathcal{D}}{\mathbb{I}(y_i = c)}
\end{align*}
where $\mathcal{D}$ is the data in the leaf node. Then information gain between the parent node $Y$ and the child node representing the test $X_j < t$ is given by []:
\begin{align*}
IG(X_j < t, Y) &= \mathbb{H}(Y) - \mathbb{H}(Y | X_j < t)\\
&=\left(-\sum_c{p(y=c) \log p(y=c)}\right) + \left(\sum_c{p(y=c|X_j < t)\log p(c|X_j < t)}\right)
\end{align*}

The motivation behind combining the results of many decision trees is that single decision trees are highly sensitive to noise in the training set and thus prone to overfitting. Combining multiple decision trees reduces the variance of the model as long as the individual trees are not correlated. This requirement is provided by building trees on different subsets of the input data and using different a subset of features on each tree.

\section{Evaluation}

We used implementations in the \texttt{scikit-learn} library for each of the classification schemes described above. Additionally, as a thought experiment, we implemented our own Bernoulli naive Bayes and multinomial naive Bayes classifiers from scratch in python and compared our results against implementations in \texttt{scikit-learn}. To enhance the robustness and generality of each of our models, we used $k$-fold cross validation with $k=5$.

\subsection{Metrics}

In addition to raw accuracy, we evaluated each of the classification methods using a number of standard classification metrics.
\begin{align*}
&\text{Area under ROC curve}\\
\text{precision} &= \dfrac{TP}{TP + FP}\\
\text{recall} &= \dfrac{TP}{TP + FN}\\
\text{F1 score} &= 2 \cdot \dfrac{\text{precision} \cdot \text{recall}}{\text{precision} + \text{recall}}
\end{align*}

These additional metrics were chosen because the raw accuracy does not contain any information about the number of false positives and false negatives. The area under the ROC curve gives us insight into the probability that a randomly chosen positive example has higher probability in being predicted as positive than a randomly chosen negative example. However, the area under two ROC curves can still be equal even if the two models have widely different precision and recall. Since precision and recall are both desirable properties, we use the F1 score, which captures the harmonic mean of both rates, in our evaluation.

\subsection{Hyperparameter tuning}

The default parameters of the decision tree and random forest classifiers in \texttt{scikit-learn} did not yield good accuracy. Thus, for these models, we experimented with different sets of hyperparameters by trial and error. More specifically, the parameter grids we explored were:
\begin{align*}
&\texttt{min\textunderscore leaf\textunderscore samples}: 1, 10, 100\\
&\texttt{max\textunderscore features}: 0.5, 0.75, 0.9\\
&\texttt{n\textunderscore estimators}: 10, 30, 100
\end{align*}
where last parameter is for random forest only. We chose to tune \texttt{min\textunderscore leaf\textunderscore samples} because the default was 1, which led to many splits that were overly specific to the training set. Similarly, tuning \texttt{max\textunderscore features} reduces the chance of overfitting. Using more trees by increasing \texttt{n\textunderscore estimators} further randomizes the training, which should lead to lower generalization errors. In our experiments, we found that the best hyperparameters to use are $\texttt{min\textunderscore leaf\textunderscore samples} = 1, \texttt{max\textunderscore features} = 0.9, \texttt{n\textunderscore estimators} = 100$ for the smaller dataset and $\texttt{min\textunderscore leaf\textunderscore samples} = 10, \texttt{max\textunderscore features} = 0.9, \texttt{n\textunderscore estimators} = 100$ for the larger dataset.

\subsection{Results}

\begin{table}[htbp]
\small
   \centering
   \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
   \hline
    &  \multicolumn{5}{c|}{2,400 sentiment labelled sentences} &
  \multicolumn{5}{c|}{10,000 Amazon reviews}  \\
  \cline{2-11}
   Classifier & Acc & Prec & Recall & $F_1$ & AUC ROC & Acc & Prec & Recall & $F_1$ & AUC ROC \\ \hline \hline
   mybnb & 0.71 & 0.67 & 0.76 & 0.71 & 0.69 & 0.51 & 0.68 & 0.27 & 0.38 & 0.55 \\\hline
   mymnb & 0.69 & 0.66 & 0.75 & 0.69 & 0.68 & 0.66 & 0.75 & 0.61 & 0.67 & 0.67 \\\hline
   bnb & 0.62 & 0.66 & 0.68 & 0.62 & 0.75 & 0.78 & 0.78 & 0.85 & 0.81 & 0.86 \\\hline
   mnb & 0.62 & 0.65 & 0.69 & 0.63 & 0.75 & 0.79 & 0.81 & 0.83 & 0.82 & 0.86 \\\hline
   dt & 0.62 & 0.62 & 0.70 & 0.63 & 0.65 & 0.70 & 0.72 & 0.78 & 0.75 & 0.75 \\\hline
   rf & 0.60 & 0.65 & 0.56 & 0.57 & 0.69 & 0.73 & 0.74 & 0.82 & 0.78 & 0.81 \\\hline
   \end{tabular}
   \caption{Accuracy, precision, recall, F1 score, and area under ROC curve for all models evaluated under two datasets of different sizes using 5-fold cross validation. \texttt{mybnb} and \texttt{mymnb} refer to the custom implementation of Bernoulli naive Bayes and multinomial naive Bayes respectively. The rest of the classifiers use implementations provided by \texttt{scikit-learn}.}
   \label{tab:classifiers}
\end{table}



\section{BLARGH}

We use ten different classification methods from the SciKitLearn Python libraries~\cite{scikit-learn}. All parameterizations are the default unless specified.
\begin{enumerate}
\item \emph{$K$-nearest neighbors} (KNN): using ten nearest neighbors and the ``KDTree" algorithm
\item \emph{Logistic regression with $\ell_2$ penalty} (LR): using stochastic gradient descent 
\item \emph{Perceptron with $\ell_2$ penalty} (P2): using stochastic gradient descent
\item \emph{Hinge loss with $\ell_2$ penalty} (HL2): using stochastic gradient descent
\item \emph{Naive Bayes classifier} (NB): using multinomial implementation
\item \emph{Support vector machine with linear kernel} (SVML): 
\item \emph{Support vector machine with squared exponential kernel} (SVMS): 
\item \emph{AdaBoost} (AB): using $100$ estimators
\item \emph{Decision tree} (DT): using Gini impurity scores 
\item \emph{Random forest} (RF): using Gini impurity scores and 100 trees
\end{enumerate}

Because this problem is one of multiclass classification, we trained and tested each of these classifiers as binary classifiers for each class using one-versus-rest classification; for this we also used the SciKit-Learn library. Because there were equal numbers of samples in each class, we averaged the evaluation metrics across the twenty classes for the values across the $20$ newsgroups.

\subsection{Evaluation}

For each classification method, we performed stratified 10-fold cross validation on the 20 Newsgroups data. We maintained the same folds across each of the classifiers. We compared the different classifier results using precision, false discovery rate (FDR), $F_1$ score, and wall clock time in seconds, all of which were averaged over the results from each of the 20 classes (each of which had identical numbers of posts). In particular, denoting the number of false positives (FPs), true positives (TPs), false negatives (FNs), true negatives (TNs), and false discovery rate (FDR), we can define precision and recall as
\begin{equation*}
\mbox{precision} = \frac {TP}{TP+FP} = 1 - FDR \mbox{,  recall} = \frac {TP}{TP+FN}
\end{equation*}
and $F_1$-score, which is the harmonic mean of precision and recall:
\begin{equation*}
F_1 = 2 \frac {precision \times recall} {precision + recall} = 2 \frac{TP}{2TP+FP+FN}.
\end{equation*}

\section{Results}

\subsection{Evaluation results}

We found variable performance of each of the ten classifiers on the 20 Newsgroups data, particularly with respect to recall (Table~\ref{tab:classifiers}). In particular, we see that the linear classifiers (LR, P2, HL2, NB, and SVML) tend to perform worse on this task on average relative to the non-linear classifiers (AB, SVMS, DT, RF), although logistic regression has strong performance for a linear classifier. Of the non-linear classifiers, the AdaBoost, random forest, and decision tree classifiers have stronger performance than the SVMS. Overall, the decision tree shows competitive performance, despite its simplicity relative to the other nonlinear methods. This suggests that it is a non-additive combination of the word counts, rather than either a very high dimensional basis function (SVMS) or ensemble classifiers (RF, AdaBoost), that creates the biggest gains in performance over linear classifiers.

For each of the one-versus-rest classification problems, we extracted the \emph{Gini} impurity scores from a trained random forest classifier. Gini impurity for a particular feature represents the information gain, or the average reduction in entropy of the classifier before and after a that feature is used across all of the trees in the random forest; a larger value indicates greater predictive power. We found that the top ten words, with respect to Gini impurity, for each class (Table~\ref{tab:words}) were representative of the topics discussed in that newsgroup (e.g., {\tt talk.religion.misc} includes \emph{moral} and \emph{religion}); there were also a number of words that were in the top ten word lists for a number of newsgroup classes, suggesting that they were important in discriminating one of the newsgroups with a $0$ label from the newsgroup with the $1$ label (e.g., \emph{mideast} is a strong indicator of {\tt talk.politics.mideast}).

\begin{table}[htbp]
   \centering
   \tiny
   \begin{tabular}{@{}|c|c|c|c|c|c|c|@{}} % Column formatting, @{} suppresses leading/trailing space 
   \hline
   rec.motorcycles & comp.sys.mac.hardware & talk.politics.misc & soc.religion.christian & comp.graphics & sci.med & talk.religion.misc\\ \hline \hline
      Austin   &  apollo& access & apple & 3 & digest &  aft\\
        b  &  central& also & atheist & come & do & chip \\
      biblic     &  come& client & chip & get & font & mideast \\
      columbia  & handheld & crabapple & geb & govern & mchp & moral \\
      doctor &  i3150101& gateway & go & handheld & med & package\\
      ecn &  me & lc & matter & IGC & path & point\\
      motif &  patient & mideast & rec & mideast & pitch & religion\\
      reason &  q& operation & religion & sun & say & sale\\
      say &  sun & point & run & win & since & take\\
      speed &  win & take & sin & write & Toronto & (quote) \\ \hline \hline
 comp.windows.x & comp.sys.ibm.pc.hardware & talk.politics.guns & alt.atheism & comp.os.ms-windows.misc & sci.crypt & sci.space\\ \hline \hline
      come & car & astro & also & come & child & AI\\
      engr & come & baseball & ATF & distribute & Clinton & ask\\
      law & handheld & fan & atheism & drive & crime & audio\\
      mideast & i3150101 & fire & Canada & FBI & ee & device\\
      motherboard & ID & ground & internet & love & electron & larc\\
      well & me & mideast & jim & mideast & kent & monitor\\
      win & mideast & point & king & nuclear & order & oracle\\
      write & patient & take & mideast & take & printer & say\\
      x & science & we & religion & win & say & software\\
      xliv & sun & watson & take & write & sea & z\\ \hline
      \hline
 misc.forsale & rec.sport.hockey & rec.sport.baseball & sci.electronics & rec.autos & talk.politics.mideast & \\ \hline \hline
   zoo & b & b & au & Austin & April&\\
   come  & c & c & batf & cantaloup & Arizona&\\
   comp & enterpoop & clock & ci & de & cso&\\
   format & f & g & come & eng & Islam&\\
   mideast & g & Henry & crime & food & Israel&\\
   observe & HIV & HIV & effect & mideast & Michael&\\
   rate & reason & picture & mchp & motif & pain&\\
   rec & speed & reason & say & reason & point&\\
   Rutgers & tax & speed & software & say & SNI&\\ 
   sgi & widget & tax & vm & speed & tu&\\ \hline
   \end{tabular}
   \caption{{\bf Top $10$ predictive words for each of the 20 Newsgroups.} The top ten words were identified after feature selection from a fitted random forest classifier using words ranked by their Gini impurity scores.}
   \label{tab:words}
\end{table}

\subsection{Computational speed}

The variability in the time for training and testing these linear classifiers was substantial (Table~\ref{tab:classifiers}). In particular, we found that the KNN classifier, which does not perform training, takes the largest amount of time because of the all-by-all comparison that occurs during test phase. AdaBoost takes the second longest, but here the time is spent on training the weak classifiers and the weights of the linear combination of those weak classifiers. The fastest classifiers include the NB classifier, the perceptron, and the hinge loss classifier, followed by the linear SVM and then the decision tree and random forest classifiers.

\subsection{Feature selection}

These results highlight the benefits for some of the methods of reducing the number of features before training the classifiers. In particular, we found that using feature selection improved the precision for SVMS, AB, and RF classifiers, and the recall for KNN, LR, NB, SVMS, and RF classifiers. The largest improvement was for the SVMS and RF classifiers. The effect on the RF classifier might be mitigated by increasing the number of trees in the random forest for larger numbers of features, although this would slow down the training time proportionally. Across all methods, feature selection substantially improved the average wall clock time, e.g., improving the time of AdaBoost by $87.5\%$.

\section{Discussion and Conclusion}

In this work, we compared ten different classifiers to predict the newsgroup for a particular newsgroup post using bag-of-words features. We found that, considering precision, recall, and time, the decision tree and random forest classifiers showed superior performance on this task.  The effect of feature selection was mostly on the time, although the improvement in performance was substantial for the random forest classifier on this task.

There are a number of directions to go that would improve these results. First, we could expand our data set using available data from these and other related newsgroups. Second, we could consider more sophisticated features for the newsgroup posts than dictionary word counts; bi-grams, post length, or punctuation may be useful in this classification task. Third, we could use the most promising models in a more problem-tailored way. In particular, because the random forest classifier showed such promise in this task, we could consider applying it to this problem using multi class class labels instead of one-versus-rest class labels, and reducing the dimension of the feature space using supervised latent Dirichlet allocation based methods \cite{zhu2009,lacoste2009}. 

\section{References?}

[] https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences

[] 'From Group to Individual Labels using Deep Features', Kotzias et. al,. KDD 2015

[] http://www.cs.jhu.edu/~mdredze/datasets/sentiment/

[] John Blitzer, Mark Dredze, Fernando Pereira. Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification. Association of Computational Linguistics (ACL), 2007

[] John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jenn Wortman. Learning Bounds for Domain Adaptation. Neural Information Processing Systems (NIPS), 2008.

[] Mark Dredze, Koby Crammer, and Fernando Pereira. Confidence-Weighted Linear Classification. International Conference on Machine Learning (ICML), 2008.

[] https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf

[] http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=709601

[] http://hunch.net/~coms-4771/quinlan.pdf

\bibliography{ref}
\bibliographystyle{plos2015}
\end{document}

